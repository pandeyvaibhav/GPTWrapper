{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pandeyvaibhav/GPTWrapper/blob/main/Azure_Infra_V1_1_openai_api_automation_wrapper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ra6T707Kb3yw"
      },
      "source": [
        "# Open AI Automation Wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITTrR9K1bwfP"
      },
      "source": [
        "#### Install Open AI Binaries\n",
        "#### Install Langchain on top of OpenAI\n",
        "#### Install Backoff and Tenacity\n",
        "#### Install PDF readers binaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iX0p--CFYgll",
        "outputId": "4f247e38-8f11-4587-d11b-a3fb5f388642"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.28.1\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.0.312-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.21)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.5)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.43 (from langchain)\n",
            "  Downloading langsmith-0.0.43-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.6.1 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.312 langsmith-0.0.43 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
            "Collecting azure-identity\n",
            "  Downloading azure_identity-1.14.1-py3-none-any.whl (161 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.2/161.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-keyvault-secrets\n",
            "  Downloading azure_keyvault_secrets-4.7.0-py3-none-any.whl (348 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.6/348.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-core<2.0.0,>=1.11.0 (from azure-identity)\n",
            "  Downloading azure_core-1.29.4-py3-none-any.whl (192 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.4/192.4 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cryptography>=2.5 in /usr/local/lib/python3.10/dist-packages (from azure-identity) (41.0.4)\n",
            "Collecting msal<2.0.0,>=1.20.0 (from azure-identity)\n",
            "  Downloading msal-1.24.1-py2.py3-none-any.whl (95 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.0/96.0 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting msal-extensions<2.0.0,>=0.3.0 (from azure-identity)\n",
            "  Downloading msal_extensions-1.0.0-py2.py3-none-any.whl (19 kB)\n",
            "Collecting azure-common~=1.1 (from azure-keyvault-secrets)\n",
            "  Downloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
            "Collecting isodate>=0.6.1 (from azure-keyvault-secrets)\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from azure-keyvault-secrets) (4.5.0)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.10/dist-packages (from azure-core<2.0.0,>=1.11.0->azure-identity) (2.31.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from azure-core<2.0.0,>=1.11.0->azure-identity) (1.16.0)\n",
            "Collecting typing-extensions>=4.0.1 (from azure-keyvault-secrets)\n",
            "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=2.5->azure-identity) (1.16.0)\n",
            "Requirement already satisfied: PyJWT[crypto]<3,>=1.0.0 in /usr/lib/python3/dist-packages (from msal<2.0.0,>=1.20.0->azure-identity) (2.3.0)\n",
            "Collecting portalocker<3,>=1.0 (from msal-extensions<2.0.0,>=0.3.0->azure-identity)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=2.5->azure-identity) (2.21)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.11.0->azure-identity) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.11.0->azure-identity) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.11.0->azure-identity) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.11.0->azure-identity) (2023.7.22)\n",
            "Installing collected packages: azure-common, typing-extensions, portalocker, isodate, azure-core, azure-keyvault-secrets, msal, msal-extensions, azure-identity\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed azure-common-1.1.28 azure-core-1.29.4 azure-identity-1.14.1 azure-keyvault-secrets-4.7.0 isodate-0.6.1 msal-1.24.1 msal-extensions-1.0.0 portalocker-2.8.2 typing-extensions-4.8.0\n",
            "Collecting mermaid-python\n",
            "  Downloading mermaid_python-0.1-py3-none-any.whl (3.2 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from mermaid-python) (7.34.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->mermaid-python) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython->mermaid-python)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->mermaid-python) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->mermaid-python) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->mermaid-python) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->mermaid-python) (3.0.39)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->mermaid-python) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->mermaid-python) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->mermaid-python) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->mermaid-python) (4.8.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->mermaid-python) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->mermaid-python) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->mermaid-python) (0.2.8)\n",
            "Installing collected packages: jedi, mermaid-python\n",
            "Successfully installed jedi-0.19.1 mermaid-python-0.1\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-3.16.4-py3-none-any.whl (276 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.6/276.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-3.16.4\n",
            "Requirement already satisfied: langchain[llms] in /usr/local/lib/python3.10/dist-packages (0.0.312)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain[llms]) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain[llms]) (2.0.21)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain[llms]) (3.8.5)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain[llms]) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain[llms]) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain[llms]) (0.6.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain[llms]) (1.33)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.43 in /usr/local/lib/python3.10/dist-packages (from langchain[llms]) (0.0.43)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain[llms]) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain[llms]) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain[llms]) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain[llms]) (8.2.3)\n",
            "Collecting clarifai>=9.1.0 (from langchain[llms])\n",
            "  Downloading clarifai-9.9.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cohere<5,>=4 (from langchain[llms])\n",
            "  Downloading cohere-4.27-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface_hub<1,>=0 (from langchain[llms])\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting manifest-ml<0.0.2,>=0.0.1 (from langchain[llms])\n",
            "  Downloading manifest_ml-0.0.1-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nlpcloud<2,>=1 (from langchain[llms])\n",
            "  Downloading nlpcloud-1.1.44-py3-none-any.whl (4.4 kB)\n",
            "Requirement already satisfied: openai<1,>=0 in /usr/local/lib/python3.10/dist-packages (from langchain[llms]) (0.28.1)\n",
            "Collecting openlm<0.0.6,>=0.0.5 (from langchain[llms])\n",
            "  Downloading openlm-0.0.5-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: torch<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain[llms]) (2.0.1+cu118)\n",
            "Collecting transformers<5,>=4 (from langchain[llms])\n",
            "  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[llms]) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[llms]) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[llms]) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[llms]) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[llms]) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[llms]) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain[llms]) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain[llms]) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain[llms]) (1.1.3)\n",
            "Collecting clarifai-grpc>=9.8.1 (from clarifai>=9.1.0->langchain[llms])\n",
            "  Downloading clarifai_grpc-9.9.0-py3-none-any.whl (218 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.7/218.7 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tritonclient==2.34.0 (from clarifai>=9.1.0->langchain[llms])\n",
            "  Downloading tritonclient-2.34.0-py3-none-manylinux1_x86_64.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clarifai>=9.1.0->langchain[llms]) (23.2)\n",
            "Collecting tqdm==4.64.1 (from clarifai>=9.1.0->langchain[llms])\n",
            "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rich==13.4.2 (from clarifai>=9.1.0->langchain[llms])\n",
            "  Downloading rich-13.4.2-py3-none-any.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.4/239.4 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting schema==0.7.5 (from clarifai>=9.1.0->langchain[llms])\n",
            "  Downloading schema-0.7.5-py2.py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich==13.4.2->clarifai>=9.1.0->langchain[llms]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich==13.4.2->clarifai>=9.1.0->langchain[llms]) (2.16.1)\n",
            "Requirement already satisfied: contextlib2>=0.5.5 in /usr/local/lib/python3.10/dist-packages (from schema==0.7.5->clarifai>=9.1.0->langchain[llms]) (21.6.0)\n",
            "Collecting python-rapidjson>=0.9.1 (from tritonclient==2.34.0->clarifai>=9.1.0->langchain[llms])\n",
            "  Downloading python_rapidjson-1.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting backoff<3.0,>=2.0 (from cohere<5,>=4->langchain[llms])\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting fastavro==1.8.2 (from cohere<5,>=4->langchain[llms])\n",
            "  Downloading fastavro-1.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib_metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from cohere<5,>=4->langchain[llms]) (6.8.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from cohere<5,>=4->langchain[llms]) (2.0.6)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain[llms]) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain[llms]) (0.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1,>=0->langchain[llms]) (3.12.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1,>=0->langchain[llms]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1,>=0->langchain[llms]) (4.8.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain[llms]) (2.4)\n",
            "Collecting dill>=0.3.5 (from manifest-ml<0.0.2,>=0.0.1->langchain[llms])\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting redis>=4.3.1 (from manifest-ml<0.0.2,>=0.0.1->langchain[llms])\n",
            "  Downloading redis-5.0.1-py3-none-any.whl (250 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.3/250.3 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sqlitedict>=2.0.0 (from manifest-ml<0.0.2,>=0.0.1->langchain[llms])\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain[llms]) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain[llms]) (3.0.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1->langchain[llms]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1->langchain[llms]) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1->langchain[llms]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=1->langchain[llms]) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch<3,>=1->langchain[llms]) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch<3,>=1->langchain[llms]) (17.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=4->langchain[llms]) (2023.6.3)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers<5,>=4->langchain[llms])\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers<5,>=4->langchain[llms])\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.44.0 in /usr/local/lib/python3.10/dist-packages (from clarifai-grpc>=9.8.1->clarifai>=9.1.0->langchain[llms]) (1.59.0)\n",
            "Requirement already satisfied: protobuf>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from clarifai-grpc>=9.8.1->clarifai>=9.1.0->langchain[llms]) (3.20.3)\n",
            "Requirement already satisfied: googleapis-common-protos>=1.53.0 in /usr/local/lib/python3.10/dist-packages (from clarifai-grpc>=9.8.1->clarifai>=9.1.0->langchain[llms]) (1.60.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata<7.0,>=6.0->cohere<5,>=4->langchain[llms]) (3.17.0)\n",
            "Collecting huggingface_hub<1,>=0 (from langchain[llms])\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain[llms]) (1.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=1->langchain[llms]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3,>=1->langchain[llms]) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich==13.4.2->clarifai>=9.1.0->langchain[llms]) (0.1.2)\n",
            "Building wheels for collected packages: sqlitedict\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16864 sha256=78188524962de5d5d9aeacdddaff65f26075041f77539817d50fac3ec9a60f42\n",
            "  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n",
            "Successfully built sqlitedict\n",
            "Installing collected packages: sqlitedict, tqdm, schema, safetensors, redis, python-rapidjson, fastavro, dill, backoff, tritonclient, rich, openlm, nlpcloud, manifest-ml, huggingface_hub, clarifai-grpc, tokenizers, cohere, clarifai, transformers\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.1\n",
            "    Uninstalling tqdm-4.66.1:\n",
            "      Successfully uninstalled tqdm-4.66.1\n",
            "  Attempting uninstall: rich\n",
            "    Found existing installation: rich 13.6.0\n",
            "    Uninstalling rich-13.6.0:\n",
            "      Successfully uninstalled rich-13.6.0\n",
            "Successfully installed backoff-2.2.1 clarifai-9.9.1 clarifai-grpc-9.9.0 cohere-4.27 dill-0.3.7 fastavro-1.8.2 huggingface_hub-0.17.3 manifest-ml-0.0.1 nlpcloud-1.1.44 openlm-0.0.5 python-rapidjson-1.12 redis-5.0.1 rich-13.4.2 safetensors-0.4.0 schema-0.7.5 sqlitedict-2.1.0 tokenizers-0.14.1 tqdm-4.64.1 transformers-4.34.0 tritonclient-2.34.0\n",
            "Requirement already satisfied: langchain[all] in /usr/local/lib/python3.10/dist-packages (0.0.312)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (2.0.21)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (3.8.5)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (0.6.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (1.33)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.43 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (0.0.43)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (8.2.3)\n",
            "Collecting O365<3.0.0,>=2.0.26 (from langchain[all])\n",
            "  Downloading O365-2.0.31-py3-none-any.whl (164 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aleph-alpha-client<3.0.0,>=2.15.0 (from langchain[all])\n",
            "  Downloading aleph_alpha_client-2.17.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting amadeus>=8.1.0 (from langchain[all])\n",
            "  Downloading amadeus-9.0.0.tar.gz (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting arxiv<2.0,>=1.4 (from langchain[all])\n",
            "  Downloading arxiv-1.4.8-py3-none-any.whl (12 kB)\n",
            "Collecting atlassian-python-api<4.0.0,>=3.36.0 (from langchain[all])\n",
            "  Downloading atlassian_python_api-3.41.2-py3-none-any.whl (167 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.2/167.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting awadb<0.4.0,>=0.3.9 (from langchain[all])\n",
            "  Downloading awadb-0.3.10-cp310-cp310-manylinux1_x86_64.whl (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-ai-formrecognizer<4.0.0,>=3.2.1 (from langchain[all])\n",
            "  Downloading azure_ai_formrecognizer-3.3.1-py3-none-any.whl (299 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.7/299.7 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-ai-vision<0.12.0,>=0.11.1b1 (from langchain[all])\n",
            "  Downloading azure_ai_vision-0.11.1b1-py3-none-manylinux1_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-cognitiveservices-speech<2.0.0,>=1.28.0 (from langchain[all])\n",
            "  Downloading azure_cognitiveservices_speech-1.32.1-py3-none-manylinux1_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-cosmos<5.0.0,>=4.4.0b1 (from langchain[all])\n",
            "  Downloading azure_cosmos-4.5.1-py3-none-any.whl (230 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.6/230.6 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: azure-identity<2.0.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (1.14.1)\n",
            "Requirement already satisfied: beautifulsoup4<5,>=4 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (4.11.2)\n",
            "Requirement already satisfied: clarifai>=9.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (9.9.1)\n",
            "Collecting clickhouse-connect<0.6.0,>=0.5.14 (from langchain[all])\n",
            "  Downloading clickhouse_connect-0.5.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (922 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m922.7/922.7 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cohere<5,>=4 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (4.27)\n",
            "Collecting deeplake<4.0.0,>=3.6.8 (from langchain[all])\n",
            "  Downloading deeplake-3.8.0.tar.gz (567 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m567.8/567.8 kB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting docarray[hnswlib]<0.33.0,>=0.32.0 (from langchain[all])\n",
            "  Downloading docarray-0.32.1-py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting duckduckgo-search<4.0.0,>=3.8.3 (from langchain[all])\n",
            "  Downloading duckduckgo_search-3.9.3-py3-none-any.whl (25 kB)\n",
            "Collecting elasticsearch<9,>=8 (from langchain[all])\n",
            "  Downloading elasticsearch-8.10.0-py3-none-any.whl (414 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m415.0/415.0 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting esprima<5.0.0,>=4.0.1 (from langchain[all])\n",
            "  Downloading esprima-4.0.1.tar.gz (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting faiss-cpu<2,>=1 (from langchain[all])\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-api-python-client==2.70.0 (from langchain[all])\n",
            "  Downloading google_api_python_client-2.70.0-py2.py3-none-any.whl (10.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-auth<3.0.0,>=2.18.1 (from langchain[all])\n",
            "  Downloading google_auth-2.23.3-py2.py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.3/182.3 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-search-results<3,>=2 (from langchain[all])\n",
            "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gptcache>=0.1.7 (from langchain[all])\n",
            "  Downloading gptcache-0.1.42-py3-none-any.whl (131 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.5/131.5 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting html2text<2021.0.0,>=2020.1.16 (from langchain[all])\n",
            "  Downloading html2text-2020.1.16-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: huggingface_hub<1,>=0 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (0.17.3)\n",
            "Requirement already satisfied: jinja2<4,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (3.1.2)\n",
            "Collecting jq<2.0.0,>=1.4.1 (from langchain[all])\n",
            "  Downloading jq-1.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (656 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m656.0/656.0 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lancedb<0.2,>=0.1 (from langchain[all])\n",
            "  Downloading lancedb-0.1.16-py3-none-any.whl (34 kB)\n",
            "Collecting langkit<0.1.0,>=0.0.6 (from langchain[all])\n",
            "  Downloading langkit-0.0.20-py3-none-any.whl (774 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.8/774.8 kB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lark<2.0.0,>=1.1.5 (from langchain[all])\n",
            "  Downloading lark-1.1.7-py3-none-any.whl (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.9/108.9 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting libdeeplake<0.0.61,>=0.0.60 (from langchain[all])\n",
            "  Downloading libdeeplake-0.0.60-cp310-cp310-manylinux2010_x86_64.whl (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: librosa<0.11.0,>=0.10.0.post2 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (0.10.1)\n",
            "Requirement already satisfied: lxml<5.0.0,>=4.9.2 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (4.9.3)\n",
            "Requirement already satisfied: manifest-ml<0.0.2,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (0.0.1)\n",
            "Collecting marqo<2.0.0,>=1.2.4 (from langchain[all])\n",
            "  Downloading marqo-1.3.1-py3-none-any.whl (36 kB)\n",
            "Collecting momento<2.0.0,>=1.10.1 (from langchain[all])\n",
            "  Downloading momento-1.10.1-py3-none-any.whl (136 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.6/136.6 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nebula3-python<4.0.0,>=3.4.0 (from langchain[all])\n",
            "  Downloading nebula3_python-3.4.0-py3-none-any.whl (312 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.4/312.4 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting neo4j<6.0.0,>=5.8.1 (from langchain[all])\n",
            "  Downloading neo4j-5.13.0.tar.gz (192 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.3/192.3 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: networkx<4,>=2.6.3 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (3.1)\n",
            "Requirement already satisfied: nlpcloud<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (1.1.44)\n",
            "Requirement already satisfied: nltk<4,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (3.8.1)\n",
            "Collecting nomic<2.0.0,>=1.0.43 (from langchain[all])\n",
            "  Downloading nomic-1.1.14.tar.gz (31 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: openai<1,>=0 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (0.28.1)\n",
            "Requirement already satisfied: openlm<0.0.6,>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (0.0.5)\n",
            "Collecting opensearch-py<3.0.0,>=2.0.0 (from langchain[all])\n",
            "  Downloading opensearch_py-2.3.2-py2.py3-none-any.whl (327 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.3/327.3 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer-six<20221106,>=20221105 (from langchain[all])\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m111.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pexpect<5.0.0,>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (4.8.0)\n",
            "Collecting pgvector<0.2.0,>=0.1.6 (from langchain[all])\n",
            "  Downloading pgvector-0.1.8-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting pinecone-client<3,>=2 (from langchain[all])\n",
            "  Downloading pinecone_client-2.2.4-py3-none-any.whl (179 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.4/179.4 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pinecone-text<0.5.0,>=0.4.2 (from langchain[all])\n",
            "  Downloading pinecone_text-0.4.2-py3-none-any.whl (17 kB)\n",
            "Collecting psycopg2-binary<3.0.0,>=2.9.5 (from langchain[all])\n",
            "  Downloading psycopg2_binary-2.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m103.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymongo<5.0.0,>=4.3.3 (from langchain[all])\n",
            "  Downloading pymongo-4.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (671 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m671.3/671.3 kB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyowm<4.0.0,>=3.3.0 (from langchain[all])\n",
            "  Downloading pyowm-3.3.0-py3-none-any.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m109.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pypdf<4.0.0,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (3.16.4)\n",
            "Collecting pytesseract<0.4.0,>=0.3.10 (from langchain[all])\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Collecting python-arango<8.0.0,>=7.5.9 (from langchain[all])\n",
            "  Downloading python_arango-7.7.0-py3-none-any.whl (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.4/108.4 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyvespa<0.34.0,>=0.33.0 (from langchain[all])\n",
            "  Downloading pyvespa-0.33.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting qdrant-client<2.0.0,>=1.3.1 (from langchain[all])\n",
            "  Downloading qdrant_client-1.6.0-py3-none-any.whl (150 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.3/150.3 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rdflib<7.0.0,>=6.3.2 (from langchain[all])\n",
            "  Downloading rdflib-6.3.2-py3-none-any.whl (528 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m528.1/528.1 kB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting redis<5,>=4 (from langchain[all])\n",
            "  Downloading redis-4.6.0-py3-none-any.whl (241 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.1/241.1 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langchain[all])\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentence-transformers<3,>=2 (from langchain[all])\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting singlestoredb<0.8.0,>=0.7.1 (from langchain[all])\n",
            "  Downloading singlestoredb-0.7.1-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.8/216.8 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-text<3.0.0,>=2.11.0 (from langchain[all])\n",
            "  Downloading tensorflow_text-2.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tigrisdb<2.0.0,>=1.0.0b6 (from langchain[all])\n",
            "  Downloading tigrisdb-1.0.0b6-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken<0.6.0,>=0.3.2 (from langchain[all])\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (2.0.1+cu118)\n",
            "Requirement already satisfied: transformers<5,>=4 in /usr/local/lib/python3.10/dist-packages (from langchain[all]) (4.34.0)\n",
            "Collecting weaviate-client<4,>=3 (from langchain[all])\n",
            "  Downloading weaviate_client-3.24.2-py3-none-any.whl (107 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wikipedia<2,>=1 (from langchain[all])\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting wolframalpha==5.0.0 (from langchain[all])\n",
            "  Downloading wolframalpha-5.0.0-py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==2.70.0->langchain[all]) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==2.70.0->langchain[all]) (0.1.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==2.70.0->langchain[all]) (2.11.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client==2.70.0->langchain[all]) (4.1.1)\n",
            "Collecting xmltodict (from wolframalpha==5.0.0->langchain[all])\n",
            "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from wolframalpha==5.0.0->langchain[all]) (10.1.0)\n",
            "Collecting jaraco.context (from wolframalpha==5.0.0->langchain[all])\n",
            "  Downloading jaraco.context-4.3.0-py3-none-any.whl (5.3 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[all]) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[all]) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[all]) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[all]) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[all]) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain[all]) (1.3.1)\n",
            "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.10/dist-packages (from aleph-alpha-client<3.0.0,>=2.15.0->langchain[all]) (2.0.6)\n",
            "Collecting aiodns>=3.0.0 (from aleph-alpha-client<3.0.0,>=2.15.0->langchain[all])\n",
            "  Downloading aiodns-3.1.0-py3-none-any.whl (5.3 kB)\n",
            "Collecting aiohttp-retry>=2.8.3 (from aleph-alpha-client<3.0.0,>=2.15.0->langchain[all])\n",
            "  Downloading aiohttp_retry-2.8.3-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from aleph-alpha-client<3.0.0,>=2.15.0->langchain[all]) (0.14.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from aleph-alpha-client<3.0.0,>=2.15.0->langchain[all]) (4.8.0)\n",
            "Requirement already satisfied: Pillow>=9.2.0 in /usr/local/lib/python3.10/dist-packages (from aleph-alpha-client<3.0.0,>=2.15.0->langchain[all]) (9.4.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain[all]) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain[all]) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain[all]) (1.1.3)\n",
            "Collecting feedparser (from arxiv<2.0,>=1.4->langchain[all])\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deprecated (from atlassian-python-api<4.0.0,>=3.36.0->langchain[all])\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from atlassian-python-api<4.0.0,>=3.36.0->langchain[all]) (1.16.0)\n",
            "Requirement already satisfied: oauthlib in /usr/local/lib/python3.10/dist-packages (from atlassian-python-api<4.0.0,>=3.36.0->langchain[all]) (3.2.2)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from atlassian-python-api<4.0.0,>=3.36.0->langchain[all]) (1.3.1)\n",
            "Requirement already satisfied: azure-core<2.0.0,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from azure-ai-formrecognizer<4.0.0,>=3.2.1->langchain[all]) (1.29.4)\n",
            "Collecting msrest>=0.6.21 (from azure-ai-formrecognizer<4.0.0,>=3.2.1->langchain[all])\n",
            "  Downloading msrest-0.7.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: azure-common~=1.1 in /usr/local/lib/python3.10/dist-packages (from azure-ai-formrecognizer<4.0.0,>=3.2.1->langchain[all]) (1.1.28)\n",
            "Requirement already satisfied: cryptography>=2.5 in /usr/local/lib/python3.10/dist-packages (from azure-identity<2.0.0,>=1.12.0->langchain[all]) (41.0.4)\n",
            "Requirement already satisfied: msal<2.0.0,>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from azure-identity<2.0.0,>=1.12.0->langchain[all]) (1.24.1)\n",
            "Requirement already satisfied: msal-extensions<2.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from azure-identity<2.0.0,>=1.12.0->langchain[all]) (1.0.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5,>=4->langchain[all]) (2.5)\n",
            "Requirement already satisfied: clarifai-grpc>=9.8.1 in /usr/local/lib/python3.10/dist-packages (from clarifai>=9.1.0->langchain[all]) (9.9.0)\n",
            "Requirement already satisfied: tritonclient==2.34.0 in /usr/local/lib/python3.10/dist-packages (from clarifai>=9.1.0->langchain[all]) (2.34.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clarifai>=9.1.0->langchain[all]) (23.2)\n",
            "Requirement already satisfied: tqdm==4.64.1 in /usr/local/lib/python3.10/dist-packages (from clarifai>=9.1.0->langchain[all]) (4.64.1)\n",
            "Requirement already satisfied: rich==13.4.2 in /usr/local/lib/python3.10/dist-packages (from clarifai>=9.1.0->langchain[all]) (13.4.2)\n",
            "Requirement already satisfied: schema==0.7.5 in /usr/local/lib/python3.10/dist-packages (from clarifai>=9.1.0->langchain[all]) (0.7.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich==13.4.2->clarifai>=9.1.0->langchain[all]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich==13.4.2->clarifai>=9.1.0->langchain[all]) (2.16.1)\n",
            "Requirement already satisfied: contextlib2>=0.5.5 in /usr/local/lib/python3.10/dist-packages (from schema==0.7.5->clarifai>=9.1.0->langchain[all]) (21.6.0)\n",
            "Requirement already satisfied: python-rapidjson>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from tritonclient==2.34.0->clarifai>=9.1.0->langchain[all]) (1.12)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect<0.6.0,>=0.5.14->langchain[all]) (2023.7.22)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect<0.6.0,>=0.5.14->langchain[all]) (2023.3.post1)\n",
            "Collecting zstandard (from clickhouse-connect<0.6.0,>=0.5.14->langchain[all])\n",
            "  Downloading zstandard-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lz4 (from clickhouse-connect<0.6.0,>=0.5.14->langchain[all])\n",
            "  Downloading lz4-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: backoff<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from cohere<5,>=4->langchain[all]) (2.2.1)\n",
            "Requirement already satisfied: fastavro==1.8.2 in /usr/local/lib/python3.10/dist-packages (from cohere<5,>=4->langchain[all]) (1.8.2)\n",
            "Requirement already satisfied: importlib_metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from cohere<5,>=4->langchain[all]) (6.8.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain[all]) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain[all]) (0.9.0)\n",
            "Collecting boto3 (from deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading boto3-1.28.62-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from deeplake<4.0.0,>=3.6.8->langchain[all]) (8.1.7)\n",
            "Collecting pathos (from deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading pathos-0.3.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting humbug>=0.3.1 (from deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading humbug-0.3.2-py3-none-any.whl (15 kB)\n",
            "Collecting numcodecs (from deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading numcodecs-0.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyjwt in /usr/lib/python3/dist-packages (from deeplake<4.0.0,>=3.6.8->langchain[all]) (2.3.0)\n",
            "INFO: pip is looking at multiple versions of deeplake to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting deeplake<4.0.0,>=3.6.8 (from langchain[all])\n",
            "  Downloading deeplake-3.7.3.tar.gz (567 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m567.6/567.6 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading deeplake-3.7.2.tar.gz (554 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m554.7/554.7 kB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading deeplake-3.7.1.tar.gz (554 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m554.7/554.7 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting aioboto3>=10.4.0 (from deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading aioboto3-11.3.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from deeplake<4.0.0,>=3.6.8->langchain[all]) (1.5.8)\n",
            "Collecting orjson>=3.8.2 (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain[all])\n",
            "  Downloading orjson-3.9.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting types-requests>=2.28.11.6 (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain[all])\n",
            "  Downloading types_requests-2.31.0.8-py3-none-any.whl (14 kB)\n",
            "Collecting hnswlib>=0.6.2 (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain[all])\n",
            "  Downloading hnswlib-0.7.0.tar.gz (33 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: protobuf>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain[all]) (3.20.3)\n",
            "Collecting aiofiles>=23.2.1 (from duckduckgo-search<4.0.0,>=3.8.3->langchain[all])\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting httpx[brotli,http2,socks]>=0.25.0 (from duckduckgo-search<4.0.0,>=3.8.3->langchain[all])\n",
            "  Downloading httpx-0.25.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting elastic-transport<9,>=8 (from elasticsearch<9,>=8->langchain[all])\n",
            "  Downloading elastic_transport-8.4.1-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0,>=2.18.1->langchain[all]) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0,>=2.18.1->langchain[all]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0,>=2.18.1->langchain[all]) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1,>=0->langchain[all]) (3.12.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1,>=0->langchain[all]) (2023.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4,>=3->langchain[all]) (2.1.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain[all]) (2.4)\n",
            "Collecting pylance==0.5.10 (from lancedb<0.2,>=0.1->langchain[all])\n",
            "  Downloading pylance-0.5.10-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ratelimiter (from lancedb<0.2,>=0.1->langchain[all])\n",
            "  Downloading ratelimiter-1.2.0.post0-py3-none-any.whl (6.6 kB)\n",
            "Collecting retry (from lancedb<0.2,>=0.1->langchain[all])\n",
            "  Downloading retry-0.9.2-py2.py3-none-any.whl (8.0 kB)\n",
            "Collecting attr (from lancedb<0.2,>=0.1->langchain[all])\n",
            "  Downloading attr-0.3.2-py2.py3-none-any.whl (3.3 kB)\n",
            "Collecting semver (from lancedb<0.2,>=0.1->langchain[all])\n",
            "  Downloading semver-3.0.2-py3-none-any.whl (17 kB)\n",
            "Collecting pyarrow>=10 (from pylance==0.5.10->lancedb<0.2,>=0.1->langchain[all])\n",
            "  Downloading pyarrow-13.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from langkit<0.1.0,>=0.0.6->langchain[all]) (1.5.3)\n",
            "Collecting textstat<0.8.0,>=0.7.3 (from langkit<0.1.0,>=0.0.6->langchain[all])\n",
            "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting whylogs==1.2.6 (from langkit<0.1.0,>=0.0.6->langchain[all])\n",
            "  Downloading whylogs-1.2.6-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs<4.0.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from whylogs==1.2.6->langkit<0.1.0,>=0.0.6->langchain[all]) (3.11.0)\n",
            "Collecting whylabs-client<1,>=0.5.1 (from whylogs==1.2.6->langkit<0.1.0,>=0.0.6->langchain[all])\n",
            "  Downloading whylabs_client-0.5.7-py3-none-any.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.0/402.0 kB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting whylogs-sketching>=3.4.1.dev3 (from whylogs==1.2.6->langkit<0.1.0,>=0.0.6->langchain[all])\n",
            "  Downloading whylogs_sketching-3.4.1.dev3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (547 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.3/547.3 kB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa<0.11.0,>=0.10.0.post2->langchain[all]) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa<0.11.0,>=0.10.0.post2->langchain[all]) (1.11.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa<0.11.0,>=0.10.0.post2->langchain[all]) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa<0.11.0,>=0.10.0.post2->langchain[all]) (1.3.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa<0.11.0,>=0.10.0.post2->langchain[all]) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa<0.11.0,>=0.10.0.post2->langchain[all]) (0.56.4)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa<0.11.0,>=0.10.0.post2->langchain[all]) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa<0.11.0,>=0.10.0.post2->langchain[all]) (1.7.0)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa<0.11.0,>=0.10.0.post2->langchain[all]) (0.3.7)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa<0.11.0,>=0.10.0.post2->langchain[all]) (0.3)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa<0.11.0,>=0.10.0.post2->langchain[all]) (1.0.7)\n",
            "Requirement already satisfied: dill>=0.3.5 in /usr/local/lib/python3.10/dist-packages (from manifest-ml<0.0.2,>=0.0.1->langchain[all]) (0.3.7)\n",
            "Requirement already satisfied: sqlitedict>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from manifest-ml<0.0.2,>=0.0.1->langchain[all]) (2.1.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.46.0 in /usr/local/lib/python3.10/dist-packages (from momento<2.0.0,>=1.10.1->langchain[all]) (1.59.0)\n",
            "Collecting momento-wire-types<0.85.0,>=0.84.0 (from momento<2.0.0,>=1.10.1->langchain[all])\n",
            "  Downloading momento_wire_types-0.84.2-py3-none-any.whl (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyjwt (from deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: future>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from nebula3-python<4.0.0,>=3.4.0->langchain[all]) (0.18.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4,>=3->langchain[all]) (2023.6.3)\n",
            "Collecting jsonlines (from nomic<2.0.0,>=1.0.43->langchain[all])\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Collecting loguru (from nomic<2.0.0,>=1.0.43->langchain[all])\n",
            "  Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wonderwords (from nomic<2.0.0,>=1.0.43->langchain[all])\n",
            "  Downloading wonderwords-2.2.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from O365<3.0.0,>=2.0.26->langchain[all]) (2.8.2)\n",
            "Collecting tzlocal<5.0,>=4.0 (from O365<3.0.0,>=2.0.26->langchain[all])\n",
            "  Downloading tzlocal-4.3.1-py3-none-any.whl (20 kB)\n",
            "Collecting stringcase>=1.2.0 (from O365<3.0.0,>=2.0.26->langchain[all])\n",
            "  Downloading stringcase-1.2.0.tar.gz (3.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect<5.0.0,>=4.8.0->langchain[all]) (0.7.0)\n",
            "Collecting dnspython>=2.0.0 (from pinecone-client<3,>=2->langchain[all])\n",
            "  Downloading dnspython-2.4.2-py3-none-any.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mmh3<4.0.0,>=3.1.0 (from pinecone-text<0.5.0,>=0.4.2->langchain[all])\n",
            "  Downloading mmh3-3.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38 kB)\n",
            "Collecting torch<3,>=1 (from langchain[all])\n",
            "  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wget<4.0,>=3.2 (from pinecone-text<0.5.0,>=0.4.2->langchain[all])\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting geojson<3,>=2.3.0 (from pyowm<4.0.0,>=3.3.0->langchain[all])\n",
            "  Downloading geojson-2.5.0-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: PySocks<2,>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from pyowm<4.0.0,>=3.3.0->langchain[all]) (1.7.1)\n",
            "Requirement already satisfied: setuptools>=42 in /usr/local/lib/python3.10/dist-packages (from python-arango<8.0.0,>=7.5.9->langchain[all]) (67.7.2)\n",
            "Collecting docker (from pyvespa<0.34.0,>=0.33.0->langchain[all])\n",
            "  Downloading docker-6.1.3-py3-none-any.whl (148 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.1/148.1 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting grpcio-tools>=1.41.0 (from qdrant-client<2.0.0,>=1.3.1->langchain[all])\n",
            "  Downloading grpcio_tools-1.59.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: portalocker<3.0.0,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from qdrant-client<2.0.0,>=1.3.1->langchain[all]) (2.8.2)\n",
            "Collecting urllib3>=1.26 (from aleph-alpha-client<3.0.0,>=2.15.0->langchain[all])\n",
            "  Downloading urllib3-1.26.17-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/143.4 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: isodate<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from rdflib<7.0.0,>=6.3.2->langchain[all]) (0.6.1)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from rdflib<7.0.0,>=6.3.2->langchain[all]) (3.1.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers<3,>=2->langchain[all]) (0.15.2+cu118)\n",
            "Collecting sentencepiece (from sentence-transformers<3,>=2->langchain[all])\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: build in /usr/local/lib/python3.10/dist-packages (from singlestoredb<0.8.0,>=0.7.1->langchain[all]) (1.0.3)\n",
            "Collecting sqlparams (from singlestoredb<0.8.0,>=0.7.1->langchain[all])\n",
            "  Downloading sqlparams-5.1.0-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from singlestoredb<0.8.0,>=0.7.1->langchain[all]) (0.41.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain[all]) (3.0.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (0.15.0)\n",
            "Collecting tensorflow<2.15,>=2.14.0 (from tensorflow-text<3.0.0,>=2.11.0->langchain[all])\n",
            "  Downloading tensorflow-2.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.8/489.8 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch<3,>=1->langchain[all])\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch<3,>=1->langchain[all])\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch<3,>=1->langchain[all])\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch<3,>=1->langchain[all])\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=4->langchain[all]) (0.4.0)\n",
            "Collecting validators<1.0.0,>=0.21.2 (from weaviate-client<4,>=3->langchain[all])\n",
            "  Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
            "Collecting authlib<2.0.0,>=1.2.1 (from weaviate-client<4,>=3->langchain[all])\n",
            "  Downloading Authlib-1.2.1-py2.py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiobotocore[boto3]==2.6.0 (from aioboto3>=10.4.0->deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading aiobotocore-2.6.0-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.31.18,>=1.31.17 (from aiobotocore[boto3]==2.6.0->aioboto3>=10.4.0->deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading botocore-1.31.17-py3-none-any.whl (11.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.10/dist-packages (from aiobotocore[boto3]==2.6.0->aioboto3>=10.4.0->deeplake<4.0.0,>=3.6.8->langchain[all]) (1.15.0)\n",
            "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore[boto3]==2.6.0->aioboto3>=10.4.0->deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading aioitertools-0.11.0-py3-none-any.whl (23 kB)\n",
            "Collecting boto3 (from deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading boto3-1.28.17-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycares>=4.0.0 (from aiodns>=3.0.0->aleph-alpha-client<3.0.0,>=2.15.0->langchain[all])\n",
            "  Downloading pycares-4.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.7/288.7 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0 (from boto3->deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading s3transfer-0.6.2-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: googleapis-common-protos>=1.53.0 in /usr/local/lib/python3.10/dist-packages (from clarifai-grpc>=9.8.1->clarifai>=9.1.0->langchain[all]) (1.60.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=2.5->azure-identity<2.0.0,>=1.12.0->langchain[all]) (1.16.0)\n",
            "Collecting protobuf>=3.19.0 (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain[all])\n",
            "  Downloading protobuf-4.24.4-cp37-abi3-manylinux2014_x86_64.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.6/311.6 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpcore<0.19.0,>=0.18.0 (from httpx[brotli,http2,socks]>=0.25.0->duckduckgo-search<4.0.0,>=3.8.3->langchain[all])\n",
            "  Downloading httpcore-0.18.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h2<5,>=3 (from httpx[brotli,http2,socks]>=0.25.0->duckduckgo-search<4.0.0,>=3.8.3->langchain[all])\n",
            "  Downloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting brotli (from httpx[brotli,http2,socks]>=0.25.0->duckduckgo-search<4.0.0,>=3.8.3->langchain[all])\n",
            "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting socksio==1.* (from httpx[brotli,http2,socks]>=0.25.0->duckduckgo-search<4.0.0,>=3.8.3->langchain[all])\n",
            "  Downloading socksio-1.0.0-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata<7.0,>=6.0->cohere<5,>=4->langchain[all]) (3.17.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa<0.11.0,>=0.10.0.post2->langchain[all]) (0.39.1)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.18.1->langchain[all]) (0.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa<0.11.0,>=0.10.0.post2->langchain[all]) (3.2.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (16.0.6)\n",
            "Collecting ml-dtypes==0.2.0 (from tensorflow<2.15,>=2.14.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all])\n",
            "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (2.3.0)\n",
            "Collecting wrapt<2.0.0,>=1.10.10 (from aiobotocore[boto3]==2.6.0->aioboto3>=10.4.0->deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.15,>=2.14.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (0.34.0)\n",
            "Collecting tensorboard<2.15,>=2.14 (from tensorflow<2.15,>=2.14.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all])\n",
            "  Downloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow<2.15,>=2.14.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all])\n",
            "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras<2.15,>=2.14.0 (from tensorflow<2.15,>=2.14.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all])\n",
            "  Downloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyphen (from textstat<0.8.0,>=0.7.3->langkit<0.1.0,>=0.0.6->langchain[all])\n",
            "  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of types-requests to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting types-requests>=2.28.11.6 (from docarray[hnswlib]<0.33.0,>=0.32.0->langchain[all])\n",
            "  Downloading types_requests-2.31.0.7-py3-none-any.whl (14 kB)\n",
            "  Downloading types_requests-2.31.0.6-py3-none-any.whl (14 kB)\n",
            "Collecting types-urllib3 (from types-requests>=2.28.11.6->docarray[hnswlib]<0.33.0,>=0.32.0->langchain[all])\n",
            "  Downloading types_urllib3-1.26.25.14-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain[all]) (1.0.0)\n",
            "Collecting pytz-deprecation-shim (from tzlocal<5.0,>=4.0->O365<3.0.0,>=2.0.26->langchain[all])\n",
            "  Downloading pytz_deprecation_shim-0.1.0.post0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build->singlestoredb<0.8.0,>=0.7.1->langchain[all]) (1.0.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build->singlestoredb<0.8.0,>=0.7.1->langchain[all]) (2.0.1)\n",
            "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from docker->pyvespa<0.34.0,>=0.33.0->langchain[all]) (1.6.3)\n",
            "Collecting sgmllib3k (from feedparser->arxiv<2.0,>=1.4->langchain[all])\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ppft>=1.7.6.7 (from pathos->deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading ppft-1.7.6.7-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pox>=0.3.3 (from pathos->deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading pox-0.3.3-py3-none-any.whl (29 kB)\n",
            "Collecting multiprocess>=0.70.15 (from pathos->deeplake<4.0.0,>=3.6.8->langchain[all])\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting py<2.0.0,>=1.4.26 (from retry->lancedb<0.2,>=0.1->langchain[all])\n",
            "  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision (from sentence-transformers<3,>=2->langchain[all])\n",
            "  Downloading torchvision-0.16.0-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m97.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading torchvision-0.15.2-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading torchvision-0.15.1-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m106.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading torchvision-0.14.1-cp310-cp310-manylinux1_x86_64.whl (24.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=2.5->azure-identity<2.0.0,>=1.12.0->langchain[all]) (2.21)\n",
            "Collecting hyperframe<7,>=6.0 (from h2<5,>=3->httpx[brotli,http2,socks]>=0.25.0->duckduckgo-search<4.0.0,>=3.8.3->langchain[all])\n",
            "  Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
            "Collecting hpack<5,>=4.0 (from h2<5,>=3->httpx[brotli,http2,socks]>=0.25.0->duckduckgo-search<4.0.0,>=3.8.3->langchain[all])\n",
            "  Downloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore<0.19.0,>=0.18.0->httpx[brotli,http2,socks]>=0.25.0->duckduckgo-search<4.0.0,>=3.8.3->langchain[all])\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich==13.4.2->clarifai>=9.1.0->langchain[all]) (0.1.2)\n",
            "INFO: pip is looking at multiple versions of pyjwt[crypto] to determine which version is compatible with other requirements. This could take a while.\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (3.4.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow<2.15,>=2.14.0->tensorflow-text<3.0.0,>=2.11.0->langchain[all]) (3.0.0)\n",
            "Collecting tzdata (from pytz-deprecation-shim->tzlocal<5.0,>=4.0->O365<3.0.0,>=2.0.26->langchain[all])\n",
            "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: amadeus, deeplake, esprima, google-search-results, neo4j, nomic, sentence-transformers, wikipedia, hnswlib, stringcase, wget, sgmllib3k\n",
            "  Building wheel for amadeus (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for amadeus: filename=amadeus-9.0.0-py2.py3-none-any.whl size=75047 sha256=d0fb8fedf33190256952ec8965472d760eef61202233dfcd70bc6d2523eea431\n",
            "  Stored in directory: /root/.cache/pip/wheels/b0/50/ea/3417d93eee6760a945d7711333d8d42b9f482e84600ef7f711\n",
            "  Building wheel for deeplake (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deeplake: filename=deeplake-3.7.1-py3-none-any.whl size=669257 sha256=222b9b7cbcf9e4f86ed62e690460fae5c35cc67dc77fd5b8049c5f299d0abf1d\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/5a/b2/c1da29595c6a68f8cc2816ad88f1a600e44a00f26f24c7e006\n",
            "  Building wheel for esprima (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for esprima: filename=esprima-4.0.1-py3-none-any.whl size=62240 sha256=4548352d42bf3332dbb3b05a8c1c4c0151d105e294249ed206f3a88b95e12de4\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/ad/8b/afd6e521e6aaea5482b7b4665ff3ce5a92373bd285e7d3a85c\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32003 sha256=5ecf919eb1590a5f905417ee5543e041489ffaf2121af21142883360bda622c6\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/b2/c3/03302d12bb44a2cdff3c9371f31b72c0c4e84b8d2285eeac53\n",
            "  Building wheel for neo4j (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neo4j: filename=neo4j-5.13.0-py3-none-any.whl size=265313 sha256=692e315f3c93373204b5c8d96022db5fe6fa05e6bc73c59aa421031a40786fda\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1d/b6/1be3a1e9de57bc832b7fcebbbf884186d8155bb6f1cc45be99\n",
            "  Building wheel for nomic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nomic: filename=nomic-1.1.14-py3-none-any.whl size=33821 sha256=17e48bb9fd13eb5e69cbf0af695cdb142d4a8c50729f575fc85b0f7756764b4b\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/dd/9a/57a82068ce36ce73954949a52ebaa1745441728b8b0233421e\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=bc5f3d5be99a8bbef8af7f9234bafeab415abef3e94c325f8a31acecb10113d8\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=4ef69e07dfb8261405afc0cc585b4d7a6292056e14017bb273f63f61c2d66118\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hnswlib: filename=hnswlib-0.7.0-cp310-cp310-linux_x86_64.whl size=2202688 sha256=f12d0249c1342cef7f39ddf6826826304c25874c0ce89a26ec2fabe06b648580\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/ae/ec/235a682e0041fbaeee389843670581ec6c66872db856dfa9a4\n",
            "  Building wheel for stringcase (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stringcase: filename=stringcase-1.2.0-py3-none-any.whl size=3568 sha256=755e024c6195f8911487aea1b38f4e0d5c3cc9727fa8e501f8ca4c75b308ec73\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/ba/22/1a2d952a9ce8aa86e42fda41e2c87fdaf20e238c88bf8df013\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=8a535b446ab37ddb008df33062a7ea99bca63e7742b5944be53069a614e16a5f\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=68c32ce989d076845ee26289eadf95146edf2be36d95559221d5349a5bb8bbaa\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built amadeus deeplake esprima google-search-results neo4j nomic sentence-transformers wikipedia hnswlib stringcase wget sgmllib3k\n",
            "Installing collected packages: whylogs-sketching, wget, types-urllib3, stringcase, sgmllib3k, sentencepiece, ratelimiter, mmh3, geojson, faiss-cpu, esprima, brotli, attr, zstandard, xmltodict, wrapt, wonderwords, validators, urllib3, tzdata, types-requests, tensorflow-estimator, sqlparams, socksio, semver, redis, pytesseract, pyphen, pyjwt, pyarrow, py, psycopg2-binary, protobuf, ppft, pox, pgvector, orjson, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, numcodecs, neo4j, multiprocess, ml-dtypes, lz4, loguru, libdeeplake, lark, keras, jsonlines, jq, jmespath, jaraco.context, hyperframe, html2text, hpack, hnswlib, h11, feedparser, dnspython, azure-cognitiveservices-speech, azure-ai-vision, awadb, amadeus, aioitertools, aiofiles, wolframalpha, whylabs-client, textstat, retry, rdflib, pytz-deprecation-shim, pymongo, pylance, pycares, pathos, nvidia-cudnn-cu11, nebula3-python, momento-wire-types, httpcore, h2, grpcio-tools, google-auth, elastic-transport, deprecated, clickhouse-connect, botocore, arxiv, wikipedia, whylogs, tzlocal, torch, tiktoken, tigrisdb, singlestoredb, s3transfer, requests-toolbelt, pinecone-client, pdfminer-six, opensearch-py, momento, marqo, lancedb, humbug, httpx, gptcache, google-search-results, elasticsearch, docker, docarray, authlib, aiohttp-retry, aiodns, aiobotocore, weaviate-client, torchvision, pyvespa, python-arango, pyowm, O365, nomic, msrest, langkit, google-api-python-client, boto3, azure-cosmos, atlassian-python-api, tensorboard, qdrant-client, duckduckgo-search, azure-ai-formrecognizer, aleph-alpha-client, tensorflow, sentence-transformers, aioboto3, tensorflow-text, pinecone-text, deeplake\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.15.0\n",
            "    Uninstalling wrapt-1.15.0:\n",
            "      Successfully uninstalled wrapt-1.15.0\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.6\n",
            "    Uninstalling urllib3-2.0.6:\n",
            "      Successfully uninstalled urllib3-2.0.6\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.13.0\n",
            "    Uninstalling tensorflow-estimator-2.13.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.13.0\n",
            "  Attempting uninstall: redis\n",
            "    Found existing installation: redis 5.0.1\n",
            "    Uninstalling redis-5.0.1:\n",
            "      Successfully uninstalled redis-5.0.1\n",
            "  Attempting uninstall: pyjwt\n",
            "    Found existing installation: PyJWT 2.3.0\n",
            "    Uninstalling PyJWT-2.3.0:\n",
            "      Successfully uninstalled PyJWT-2.3.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 9.0.0\n",
            "    Uninstalling pyarrow-9.0.0:\n",
            "      Successfully uninstalled pyarrow-9.0.0\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.3.1\n",
            "    Uninstalling ml-dtypes-0.3.1:\n",
            "      Successfully uninstalled ml-dtypes-0.3.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.13.1\n",
            "    Uninstalling keras-2.13.1:\n",
            "      Successfully uninstalled keras-2.13.1\n",
            "  Attempting uninstall: google-auth\n",
            "    Found existing installation: google-auth 2.17.3\n",
            "    Uninstalling google-auth-2.17.3:\n",
            "      Successfully uninstalled google-auth-2.17.3\n",
            "  Attempting uninstall: tzlocal\n",
            "    Found existing installation: tzlocal 5.1\n",
            "    Uninstalling tzlocal-5.1:\n",
            "      Successfully uninstalled tzlocal-5.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1+cu118\n",
            "    Uninstalling torch-2.0.1+cu118:\n",
            "      Successfully uninstalled torch-2.0.1+cu118\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.15.2+cu118\n",
            "    Uninstalling torchvision-0.15.2+cu118:\n",
            "      Successfully uninstalled torchvision-0.15.2+cu118\n",
            "  Attempting uninstall: google-api-python-client\n",
            "    Found existing installation: google-api-python-client 2.84.0\n",
            "    Uninstalling google-api-python-client-2.84.0:\n",
            "      Successfully uninstalled google-api-python-client-2.84.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.13.0\n",
            "    Uninstalling tensorboard-2.13.0:\n",
            "      Successfully uninstalled tensorboard-2.13.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.13.0\n",
            "    Uninstalling tensorflow-2.13.0:\n",
            "      Successfully uninstalled tensorflow-2.13.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires google-auth==2.17.3, but you have google-auth 2.23.3 which is incompatible.\n",
            "pandas-gbq 0.17.9 requires pyarrow<10.0dev,>=3.0.0, but you have pyarrow 13.0.0 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.24.4 which is incompatible.\n",
            "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed O365-2.0.31 aioboto3-11.3.0 aiobotocore-2.6.0 aiodns-3.1.0 aiofiles-23.2.1 aiohttp-retry-2.8.3 aioitertools-0.11.0 aleph-alpha-client-2.17.0 amadeus-9.0.0 arxiv-1.4.8 atlassian-python-api-3.41.2 attr-0.3.2 authlib-1.2.1 awadb-0.3.10 azure-ai-formrecognizer-3.3.1 azure-ai-vision-0.11.1b1 azure-cognitiveservices-speech-1.32.1 azure-cosmos-4.5.1 boto3-1.28.17 botocore-1.31.17 brotli-1.1.0 clickhouse-connect-0.5.25 deeplake-3.7.1 deprecated-1.2.14 dnspython-2.4.2 docarray-0.32.1 docker-6.1.3 duckduckgo-search-3.9.3 elastic-transport-8.4.1 elasticsearch-8.10.0 esprima-4.0.1 faiss-cpu-1.7.4 feedparser-6.0.10 geojson-2.5.0 google-api-python-client-2.70.0 google-auth-2.23.3 google-search-results-2.4.2 gptcache-0.1.42 grpcio-tools-1.59.0 h11-0.14.0 h2-4.1.0 hnswlib-0.7.0 hpack-4.0.0 html2text-2020.1.16 httpcore-0.18.0 httpx-0.25.0 humbug-0.3.2 hyperframe-6.0.1 jaraco.context-4.3.0 jmespath-1.0.1 jq-1.6.0 jsonlines-4.0.0 keras-2.14.0 lancedb-0.1.16 langkit-0.0.20 lark-1.1.7 libdeeplake-0.0.60 loguru-0.7.2 lz4-4.3.2 marqo-1.3.1 ml-dtypes-0.2.0 mmh3-3.1.0 momento-1.10.1 momento-wire-types-0.84.2 msrest-0.7.1 multiprocess-0.70.15 nebula3-python-3.4.0 neo4j-5.13.0 nomic-1.1.14 numcodecs-0.12.0 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 opensearch-py-2.3.2 orjson-3.9.8 pathos-0.3.1 pdfminer-six-20221105 pgvector-0.1.8 pinecone-client-2.2.4 pinecone-text-0.4.2 pox-0.3.3 ppft-1.7.6.7 protobuf-4.24.4 psycopg2-binary-2.9.9 py-1.11.0 pyarrow-13.0.0 pycares-4.4.0 pyjwt-2.8.0 pylance-0.5.10 pymongo-4.5.0 pyowm-3.3.0 pyphen-0.14.0 pytesseract-0.3.10 python-arango-7.7.0 pytz-deprecation-shim-0.1.0.post0 pyvespa-0.33.0 qdrant-client-1.6.0 ratelimiter-1.2.0.post0 rdflib-6.3.2 redis-4.6.0 requests-toolbelt-1.0.0 retry-0.9.2 s3transfer-0.6.2 semver-3.0.2 sentence-transformers-2.2.2 sentencepiece-0.1.99 sgmllib3k-1.0.0 singlestoredb-0.7.1 socksio-1.0.0 sqlparams-5.1.0 stringcase-1.2.0 tensorboard-2.14.1 tensorflow-2.14.0 tensorflow-estimator-2.14.0 tensorflow-text-2.14.0 textstat-0.7.3 tigrisdb-1.0.0b6 tiktoken-0.5.1 torch-1.13.1 torchvision-0.14.1 types-requests-2.31.0.6 types-urllib3-1.26.25.14 tzdata-2023.3 tzlocal-4.3.1 urllib3-1.26.17 validators-0.22.0 weaviate-client-3.24.2 wget-3.2 whylabs-client-0.5.7 whylogs-1.2.6 whylogs-sketching-3.4.1.dev3 wikipedia-1.4.0 wolframalpha-5.0.0 wonderwords-2.2.0 wrapt-1.14.1 xmltodict-0.13.0 zstandard-0.21.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install --upgrade openai\n",
        "#For Fine tuning. Work is pending.\n",
        "#!pip install --upgrade openai wandb\n",
        "#Add Langchain\n",
        "!pip install langchain\n",
        "!pip install azure-identity azure-keyvault-secrets\n",
        "!pip install mermaid-python\n",
        "\n",
        "#These are used only for reading from documents like PDF, etc.\n",
        "!pip install pypdf\n",
        "!pip install langchain[llms]\n",
        "!pip install langchain[all]\n",
        "\n",
        "from mermaid import Mermaid\n",
        "import openai\n",
        "import os\n",
        "import json\n",
        "import pprint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3GfZL5Yie4x"
      },
      "source": [
        "## Selecting required binaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DE5xziKliVCa"
      },
      "outputs": [],
      "source": [
        "#Langchain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.output_parsers import ResponseSchema\n",
        "from langchain.output_parsers import StructuredOutputParser\n",
        "\n",
        "#Building Memory\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBUht2eDh3Lr"
      },
      "source": [
        "## Langchain and PDF reading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "25Q-HddVbjf_"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.document_loaders import PyPDFLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YMnIDN15rf-7"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EI-HhqjYYcXX"
      },
      "outputs": [],
      "source": [
        "from tenacity import (\n",
        "    retry,\n",
        "    stop_after_attempt,\n",
        "    wait_random_exponential,\n",
        ")  # for exponential backoff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwy26ScRZLu2"
      },
      "source": [
        "#Common Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCzYXUewYw21"
      },
      "source": [
        "### Common Variables - Path for drives."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "d3xP8TqNYxXa"
      },
      "outputs": [],
      "source": [
        "##Set this variable for your inputs.\n",
        "drive_path1 = '/gdrive/MyDrive/Gen-AI/Input'\n",
        "local_path = '/content'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5qTDvv3aW5Z"
      },
      "source": [
        "#### Variable to hold model answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jP3UvqYndwKv"
      },
      "outputs": [],
      "source": [
        "#holds responses from the model\n",
        "response_list = []\n",
        "prompts = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tbhfYrSM9Os"
      },
      "source": [
        "Secrets Setting - Replace the secret Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "r5RHmuXcM7W-"
      },
      "outputs": [],
      "source": [
        "#This can be read from a secrets location like Azure Keyvault or it can be read from app secrets in case of an Azure Function type application.\n",
        "openai.api_key  = ''\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOryKkCCG3KB"
      },
      "source": [
        "## Common Methods Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He1LQvEIZWG1"
      },
      "source": [
        "### Setup google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fB6jhURCZPKQ",
        "outputId": "7faa6d29-1fa4-4f50-d638-937614049bcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "Drive Path:/gdrive/MyDrive/Gen-AI/Input\n"
          ]
        }
      ],
      "source": [
        "#Mounting Google drive. You can add your favourite drive/storage option.\n",
        "def mount_google_drive():\n",
        "  # Mount Google Drive\n",
        "  drive.mount('/gdrive', force_remount=True)\n",
        "  print(\"Drive Path:\" + drive_path1)\n",
        "#Attempt loading google drive\n",
        "mount_google_drive()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xd-Cf9ccYAuW",
        "outputId": "3fa238de-3b88-4cd1-fce3-c20012986709"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BackUp\tCode-C#  PDF_Requirements  Recipe\n"
          ]
        }
      ],
      "source": [
        "#print files in the drive for test!!\n",
        "!ls {drive_path1}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvrxCFRZlK6o"
      },
      "source": [
        "### Building Prompt template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tZBaSy-ENwUn"
      },
      "outputs": [],
      "source": [
        "# Pass on a role, context and action needed from the model.\n",
        "# Roles sets the thinking hat model neeeds to put on.\n",
        "# Context gives it information to be used for the current problem domain.\n",
        "# Prompt - is the action you want model to take on the problem domain.\n",
        "def make_prompt(role_for_model, context_for_model, prompt):\n",
        "  prompt1 = f\"\"\"\n",
        "  ```Think like a/an {role_for_model} and and be as accurate as possible. {context_for_model} and {prompt} ```\n",
        "  \"\"\"\n",
        "  print(prompt1)\n",
        "  return prompt1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcNSkYxWUOCE"
      },
      "source": [
        "### GPT 3.5 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_7zcnyEUNVi",
        "outputId": "596c3bb6-1845-4085-ed5e-08f8b2e80498"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text-search-babbage-doc-001\n",
            "Hello! How can I assist you today?\n"
          ]
        }
      ],
      "source": [
        "# list models\n",
        "models = openai.Model.list()\n",
        "\n",
        "# print the first model's id\n",
        "print(models.data[0].id)\n",
        "\n",
        "# create a chat completion\n",
        "chat_completion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=[{\"role\": \"user\", \"content\": \"Hello world\"}])\n",
        "\n",
        "# print the chat completion\n",
        "print(chat_completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGs7zSvbV3kT"
      },
      "source": [
        "### Adding tenacity with retry whenwe hit rate limiting from OpenAI\n",
        "https://cookbook.openai.com/examples/how_to_handle_rate_limits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzuZzyzAT0Z0"
      },
      "source": [
        "### Setting up GPT Model 3.5 for calls. This is the usual one available via ChatGPT interface. Use this when possible as it is cheaper and gives good solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2_vm6C6b33Hq"
      },
      "outputs": [],
      "source": [
        "from tenacity import (\n",
        "    retry,\n",
        "    stop_after_attempt,\n",
        "    wait_random_exponential,\n",
        ")  # for exponential backoff\n",
        "\n",
        "prompts = []\n",
        "\n",
        "## Method with retry\n",
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
        "def get_response_from_model_3_5(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0.0, # this is the degree of randomness of the model's output\n",
        "    )\n",
        "    #print(response.choices[0].message[\"content\"])\n",
        "    return response.choices[0].message[\"content\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVFI0AL-TmaC"
      },
      "source": [
        "### Setting up GPT Model 4 for calls. This is only available as paid option for anyone and is atleast 30 times more costly for same type of prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "gwNw_ynASar6",
        "outputId": "845c9876-1c82-4c9c-e2fd-521f15cef96b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! How can I assist you today?\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello! How can I assist you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "from tenacity import (\n",
        "    retry,\n",
        "    stop_after_attempt,\n",
        "    wait_random_exponential,\n",
        ")  # for exponential backoff\n",
        "\n",
        "\n",
        "## Method with retry to handle rate limitting\n",
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
        "def get_response_from_model_4_0(prompt, model=\"gpt-4\"):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0.0, # this is the degree of randomness of the model's output\n",
        "    )\n",
        "    print(response.choices[0].message[\"content\"])\n",
        "    return response.choices[0].message[\"content\"]\n",
        "\n",
        "get_response_from_model_4_0(\"Hello\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_s45HaezCbn0"
      },
      "source": [
        "### Read input recipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "bmlMYNiICa0S"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "def read_input_recepie(path):\n",
        "  print(\"read_input_recepie processing \")\n",
        "  json_data = \"\"\n",
        "  mount_google_drive()\n",
        "  print(path)\n",
        "  # Open and read the JSON file\n",
        "  if os.path.isfile(path):\n",
        "    with open(path, 'r') as json_file:\n",
        "      #print(json_file)\n",
        "      json_data = json.load(json_file)\n",
        "      json_string = json.dumps(json_data)\n",
        "\n",
        "  #You can access and work with the data as needed. For example, you can print the content:\n",
        "  print(\"read_input_recepie processing finished:-\\n\" + json.dumps(json_data))\n",
        "  #print(type (json.dumps(json_data)))\n",
        "  print(type(json.dumps(json_data)))\n",
        "  return json.dumps(json_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def is_valid_json(text):\n",
        "    try:\n",
        "        json.loads(text)\n",
        "        return True\n",
        "    except json.JSONDecodeError:\n",
        "        return False\n",
        "\n",
        "print(is_valid_json(\"sdfsdfsdfsdf\"))\n",
        "\n",
        "#Test code.\n",
        "ans = read_input_recepie(\"/gdrive/MyDrive/Gen-AI/Input/Recipe/InfraArchitect_recipe.json\")\n",
        "print(is_valid_json(ans))"
      ],
      "metadata": {
        "id": "oI2p3Rf-8JY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "bTKIMOUYqzXg"
      },
      "outputs": [],
      "source": [
        "#General method to read a file\n",
        "def read_file(path):\n",
        "  mount_google_drive()\n",
        "\n",
        "  if os.path.isfile(path):\n",
        "    with open(path, 'r') as file:\n",
        "     file_data = file.read()\n",
        "    return file_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONyxmSS6qUIv"
      },
      "outputs": [],
      "source": [
        "#Test call.\n",
        "pprint.pprint(read_file('/gdrive/MyDrive/Gen-AI/Input/Recipe/Person.json'), width=90)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPtkKSABEwmt"
      },
      "source": [
        "### Iterate the files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPPuRBPxEv4V"
      },
      "outputs": [],
      "source": [
        "mount_google_drive()\n",
        "\n",
        "#Get all the files in a folder.\n",
        "def iterate_files_content():\n",
        "  all_items = os.listdir(drive_path1)\n",
        "\n",
        "  for file_name in all_items:\n",
        "    file_path = os.path.join(drive_path1,file_name )\n",
        "    #print(file_path)\n",
        "    pprint.pprint(read_file(file_path), width=90)\n",
        "    #print(file_name)\n",
        "\n",
        "  #Filter only the files (exclude directories)\n",
        "  #files = [item for item in all_items if os.path.isfile(os.path.join(drive_path1, item))]\n",
        "  #Iterate through the files\n",
        "  #for file_name in files:\n",
        "    #print(file_name)\n",
        "\n",
        "#Checking files present.\n",
        "iterate_files_content()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUuFjVMJpvQO"
      },
      "source": [
        "## Parse the output JSON from Open AI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flEGsLmSC6ZK",
        "outputId": "5dadadde-1b2a-43f1-f67d-5131ed88fef4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "parse_json processing\n",
            "<class 'str'>\n",
            "John\n"
          ]
        }
      ],
      "source": [
        "#Parse a JSON\n",
        "def parse_json(json_string):\n",
        "    print(\"parse_json processing\")\n",
        "    print(type(json_string))\n",
        "    try:\n",
        "        # Parse the JSON string into a Python object (usually a dictionary or list)\n",
        "        parsed_data = json.loads(json_string)\n",
        "        return parsed_data\n",
        "    except json.JSONDecodeError as e:\n",
        "        # Handle JSON decoding errors, such as invalid JSON syntax\n",
        "        print(f\"JSON parsing error: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example usage: Test Case\n",
        "json_string = '{\"name\": \"John\", \"age\": 30, \"city\": \"New York\"}'\n",
        "parsed_data = parse_json(json_string)\n",
        "if parsed_data:\n",
        "    # Access and work with the parsed data\n",
        "    #print(parsed_data[0].name)\n",
        "    print(parsed_data.get(\"name\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ans = read_input_recepie(\"/gdrive/MyDrive/Gen-AI/Input/Recipe/InfraArchitect_recipe.json\")\n",
        "parsed_ans = parse_json(ans)\n",
        "#print(is_valid_json(parsed_ans))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOmOn5_k-6zJ",
        "outputId": "880f7d8d-f488-4656-94be-c218ccda1678"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "read_input_recepie processing \n",
            "Mounted at /gdrive\n",
            "Drive Path:/gdrive/MyDrive/Gen-AI/Input\n",
            "/gdrive/MyDrive/Gen-AI/Input/Recipe/InfraArchitect_recipe.json\n",
            "read_input_recepie processing finished:-\n",
            "{\"Role\": \"Cloud Infrastructure Architect\", \"Context\": \"I have two systems, one of them is SAP, and another one is Commercetools. SAP will emit files, and Commercetools will accept messages using C# SDK. We are looking at integrating them. SAP files will have multiple records. Consider usage of Azure-based cloud platform with PAAS based offering mainly utilising API Management, Service Bus messaging, Azure Function Apps\", \"Prompts\": [\"Generate a Terraform Code to create Azure Service Bus, Azure API Management, Azure Storage, Azure Function App and hosted in a VNet with appropriate Subnets\"]}\n",
            "<class 'str'>\n",
            "parse_json processing\n",
            "<class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpOhGBmUp0Pk"
      },
      "source": [
        "## Reading Recipe/es one by one depending on the function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "radWCoHMZN4g"
      },
      "outputs": [],
      "source": [
        "#path = full_path = os.path.join(drive_path1, \"Input\", \"Cloud_Arch_Recipe.json\")\n",
        "#data = parse_json(read_input_recepie(path))\n",
        "#role_for_model = data[\"Role\"]\n",
        "#context_for_model = data[\"Context\"]\n",
        "#prompts = data[\"Prompts\"]\n",
        "#print(role_for_model)\n",
        "#print(context)\n",
        "#print(type(prompts))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEvjyAsIp_Gi"
      },
      "source": [
        "## Debug method to calculate if prompt may be hitting token(context) ceilings for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "OV1gffFtOt83"
      },
      "outputs": [],
      "source": [
        "def count_words(input_string):\n",
        "    #print(input_string)\n",
        "    #print(type(input_string))\n",
        "    words = input_string.split()\n",
        "    word_count = len(words)\n",
        "    #if word_count > 850:\n",
        "      #print(input_string)\n",
        "      #print(\"Inaccurate or incomplete response is expected as we may be breaching token limit\")\n",
        "    return len(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNlgxzdLqH2g"
      },
      "source": [
        "## Emit Model Optput for local debugging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "B_OT6Vu-mGFV"
      },
      "outputs": [],
      "source": [
        "import markdown\n",
        "from IPython.display import HTML\n",
        "\n",
        "#Emit the model responses.\n",
        "def emit_output(response_list):\n",
        "  for item in response_list:\n",
        "    print(item)\n",
        "    #pprint.pprint(item, width=180)\n",
        "    #print(markdown.markdown(item))\n",
        "    #HTML(markdown.markdown(item))\n",
        "    #html_text = markdown.markdown(item)\n",
        "    # Display the HTML\n",
        "    #HTML(html_text)\n",
        "\n",
        "emit_output(response_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uU3A-TLdwF65"
      },
      "source": [
        "Setting the role for the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEX3rnOum6mC",
        "outputId": "789165e8-3037-4086-8044-2c67caf671a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "recipe_filename:-Recipe/InfraArchitect_recipe.json\n",
            "Basepath:-/gdrive/MyDrive/Gen-AI/Input\n",
            "Joining path\n",
            "Path:- /gdrive/MyDrive/Gen-AI/Input/Recipe/InfraArchitect_recipe.json\n",
            "read_input_recepie processing \n",
            "Mounted at /gdrive\n",
            "Drive Path:/gdrive/MyDrive/Gen-AI/Input\n",
            "/gdrive/MyDrive/Gen-AI/Input/Recipe/InfraArchitect_recipe.json\n",
            "read_input_recepie processing finished:-\n",
            "{\"Role\": \"Cloud Infrastructure Architect\", \"Context\": \"I have two systems, one of them is SAP, and another one is Commercetools. SAP will emit files, and Commercetools will accept messages using C# SDK. We are looking at integrating them. SAP files will have multiple records. Consider usage of Azure-based cloud platform with PAAS based offering mainly utilising API Management, Service Bus messaging, Azure Function Apps\", \"Prompts\": [\"Generate a Terraform Code to create Azure Service Bus, Azure API Management, Azure Storage, Azure Function App and hosted in a VNet with appropriate Subnets\"]}\n",
            "<class 'str'>\n",
            "parse_json processing\n",
            "<class 'str'>\n",
            "Mounted at /gdrive\n",
            "Drive Path:/gdrive/MyDrive/Gen-AI/Input\n",
            "/gdrive/MyDrive/Gen-AI/Input/Recipe/InfraArchitect_recipe.json\n",
            "{'Role': 'Cloud Infrastructure Architect', 'Context': 'I have two systems, one of them is SAP, and another one is Commercetools. SAP will emit files, and Commercetools will accept messages using C# SDK. We are looking at integrating them. SAP files will have multiple records. Consider usage of Azure-based cloud platform with PAAS based offering mainly utilising API Management, Service Bus messaging, Azure Function Apps', 'Prompts': ['Generate a Terraform Code to create Azure Service Bus, Azure API Management, Azure Storage, Azure Function App and hosted in a VNet with appropriate Subnets']}\n",
            "Cloud Infrastructure Architect\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Cloud Infrastructure Architect',\n",
              " 'I have two systems, one of them is SAP, and another one is Commercetools. SAP will emit files, and Commercetools will accept messages using C# SDK. We are looking at integrating them. SAP files will have multiple records. Consider usage of Azure-based cloud platform with PAAS based offering mainly utilising API Management, Service Bus messaging, Azure Function Apps',\n",
              " ['Generate a Terraform Code to create Azure Service Bus, Azure API Management, Azure Storage, Azure Function App and hosted in a VNet with appropriate Subnets'])"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ],
      "source": [
        "def build_recipe_path( recipe_filename, basepath):\n",
        "  print(\"recipe_filename:-\" + recipe_filename)\n",
        "  print(\"Basepath:-\" + basepath)\n",
        "\n",
        "  print(\"Joining path\")\n",
        "  path = os.path.join(basepath, recipe_filename)\n",
        "  print(\"Path:- \"+path)\n",
        "  data = parse_json(read_input_recepie(path))\n",
        "  print(load_json_and_access_role(path))\n",
        "\n",
        "  #print(\"Checking if its valid json response?\")\n",
        "  #print(is_valid_json(data))\n",
        "\n",
        "  #print(data)\n",
        "  #%debug\n",
        "\n",
        "  role_for_model = data[\"Role\"]\n",
        "  context_for_model = data[\"Context\"]\n",
        "  prompts = data[\"Prompts\"]\n",
        "  #Debug prompts\n",
        "  #print(role_for_model)\n",
        "  #print(context_for_model)\n",
        "  #print(\"printing prompts:\")\n",
        "  #print(prompts)\n",
        "  return  role_for_model, context_for_model, prompts\n",
        "\n",
        "#test the logic:\n",
        "build_recipe_path(\"Recipe/InfraArchitect_recipe.json\", drive_path1 )\n",
        "\n",
        "#/gdrive/MyDrive/Gen-AI/Input/Recipe/Infra_Architect_recipe.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "Qc1ofbi4qG9s"
      },
      "outputs": [],
      "source": [
        "#Read files in a folder\n",
        "#def read_files(path):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AccFGoZudjiG"
      },
      "source": [
        "## Setting up Orchestration Methods on top of OpenAIs GPT 3.5 turbo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MubqaGnQdsyU"
      },
      "source": [
        "### Orchestration Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "sfVCDlkzsZj_"
      },
      "outputs": [],
      "source": [
        "#Get the recipe file and invoke the model for prompt processing.\n",
        "def build_and_act_3_5(recipe_filename, basepath=drive_path1, directory=\"Input\", ):\n",
        "  print(recipe_filename)\n",
        "  print(basepath)\n",
        "  print(directory)\n",
        "  role_for_model, context_for_model, prompts = build_recipe_path(recipe_filename,basepath)\n",
        "  act_on_prompts_3_5(role_for_model, context_for_model, prompts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTeXfwcidx1E"
      },
      "source": [
        "### Action method - invokes the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "VHHae7xBshwK"
      },
      "outputs": [],
      "source": [
        "def act_on_prompts_3_5(role_for_model, context_for_model, prompts):\n",
        "    print(prompts)\n",
        "    for item in prompts:\n",
        "      print(item)\n",
        "      #build individual prompts\n",
        "      #count_words(make_prompt(role_for_model, context_for_model, item))\n",
        "      response_list.append(get_response_from_model_3_5(make_prompt(role_for_model, context_for_model, item)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSNK-cpChhPu"
      },
      "source": [
        "## Test the recipe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSujEMQtlnBz",
        "outputId": "d9f017d3-df17-4ed2-a0df-ee8b3cfac0d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recipe/InfraArchitect_recipe.json\n",
            "/gdrive/MyDrive/Gen-AI/Input\n",
            "Input\n",
            "recipe_filename:-Recipe/InfraArchitect_recipe.json\n",
            "Basepath:-/gdrive/MyDrive/Gen-AI/Input\n",
            "Joining path\n",
            "Path:- /gdrive/MyDrive/Gen-AI/Input/Recipe/InfraArchitect_recipe.json\n",
            "read_input_recepie processing \n",
            "Mounted at /gdrive\n",
            "Drive Path:/gdrive/MyDrive/Gen-AI/Input\n",
            "/gdrive/MyDrive/Gen-AI/Input/Recipe/InfraArchitect_recipe.json\n",
            "read_input_recepie processing finished:-\n",
            "{\"Role\": \"Cloud Infrastructure Architect\", \"Context\": \"I have two systems, one of them is SAP, and another one is Commercetools. SAP will emit files, and Commercetools will accept messages using C# SDK. We are looking at integrating them. SAP files will have multiple records. Consider usage of Azure-based cloud platform with PAAS based offering mainly utilising API Management, Service Bus messaging, Azure Function Apps\", \"Prompts\": [\"Generate a Terraform Code to create Azure Service Bus, Azure API Management, Azure Storage, Azure Function App and hosted in a VNet with appropriate Subnets\"]}\n",
            "<class 'str'>\n",
            "parse_json processing\n",
            "<class 'str'>\n",
            "Mounted at /gdrive\n",
            "Drive Path:/gdrive/MyDrive/Gen-AI/Input\n",
            "/gdrive/MyDrive/Gen-AI/Input/Recipe/InfraArchitect_recipe.json\n",
            "{'Role': 'Cloud Infrastructure Architect', 'Context': 'I have two systems, one of them is SAP, and another one is Commercetools. SAP will emit files, and Commercetools will accept messages using C# SDK. We are looking at integrating them. SAP files will have multiple records. Consider usage of Azure-based cloud platform with PAAS based offering mainly utilising API Management, Service Bus messaging, Azure Function Apps', 'Prompts': ['Generate a Terraform Code to create Azure Service Bus, Azure API Management, Azure Storage, Azure Function App and hosted in a VNet with appropriate Subnets']}\n",
            "Cloud Infrastructure Architect\n",
            "['Generate a Terraform Code to create Azure Service Bus, Azure API Management, Azure Storage, Azure Function App and hosted in a VNet with appropriate Subnets']\n",
            "Generate a Terraform Code to create Azure Service Bus, Azure API Management, Azure Storage, Azure Function App and hosted in a VNet with appropriate Subnets\n",
            "\n",
            "  ```Think like a/an Cloud Infrastructure Architect and and be as accurate as possible. I have two systems, one of them is SAP, and another one is Commercetools. SAP will emit files, and Commercetools will accept messages using C# SDK. We are looking at integrating them. SAP files will have multiple records. Consider usage of Azure-based cloud platform with PAAS based offering mainly utilising API Management, Service Bus messaging, Azure Function Apps and Generate a Terraform Code to create Azure Service Bus, Azure API Management, Azure Storage, Azure Function App and hosted in a VNet with appropriate Subnets ```\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "build_and_act_3_5(\"Recipe/InfraArchitect_recipe.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "8xK5uZuosgy_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bacefbd-5f44-421e-a3c3-3480c95d3a98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As a Cloud Infrastructure Architect, I would design the integration between SAP and Commercetools using the Azure-based cloud platform with PAAS offerings. Here is a high-level overview of the architecture:\n",
            "\n",
            "1. SAP Integration:\n",
            "   - SAP will emit files containing multiple records.\n",
            "   - These files can be stored in Azure Storage for further processing.\n",
            "   - Azure Function Apps can be used to monitor the storage container and trigger processing whenever new files are added.\n",
            "   - The Function App can read the SAP files, extract the necessary data, and transform it into a format suitable for integration with Commercetools.\n",
            "   - The transformed data can be sent to the Service Bus messaging system for further processing.\n",
            "\n",
            "2. Commercetools Integration:\n",
            "   - The C# SDK for Commercetools can be used to accept messages from the Service Bus.\n",
            "   - Azure Function Apps can be used to process the messages received from the Service Bus and perform any necessary actions in Commercetools.\n",
            "   - The Function App can utilize the C# SDK to interact with the Commercetools API and perform operations such as creating products, updating inventory, etc.\n",
            "\n",
            "3. API Management:\n",
            "   - Azure API Management can be used to expose APIs for both SAP and Commercetools integration.\n",
            "   - It can provide a unified interface for external systems to interact with the integrated solution.\n",
            "   - API Management can handle authentication, authorization, rate limiting, and other API management functionalities.\n",
            "\n",
            "4. Networking and Security:\n",
            "   - The entire solution can be hosted within a Virtual Network (VNet) for enhanced security and isolation.\n",
            "   - The VNet can be divided into appropriate subnets to segregate different components of the solution.\n",
            "   - Network Security Groups (NSGs) can be used to control inbound and outbound traffic to the subnets.\n",
            "   - Access to the resources within the VNet can be further restricted using Azure Private Link.\n",
            "\n",
            "To generate a Terraform code for creating the required Azure resources, you can use the following steps:\n",
            "\n",
            "1. Define the Azure Resource Group and location.\n",
            "2. Create an Azure Storage Account for storing SAP files.\n",
            "3. Create an Azure Function App for processing SAP files and sending messages to the Service Bus.\n",
            "4. Create an Azure Service Bus Namespace and Queue/Topic for message communication.\n",
            "5. Create an Azure Function App for processing messages from the Service Bus and interacting with Commercetools.\n",
            "6. Create an Azure API Management instance and configure APIs for SAP and Commercetools integration.\n",
            "7. Configure the VNet and subnets for hosting the solution components.\n",
            "8. Associate the Function Apps and API Management with the appropriate subnets.\n",
            "9. Configure NSGs and Azure Private Link for network security and isolation.\n",
            "\n",
            "By following these steps, you can create the necessary Azure resources using Terraform to implement the integration between SAP and Commercetools on the Azure cloud platform.\n"
          ]
        }
      ],
      "source": [
        "emit_output(response_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jy12g9yIRXY5"
      },
      "source": [
        "## Setting-up Orchestration methods to call GPT Model 4\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IUC0-yxbI6j"
      },
      "source": [
        "#### Orchestration Method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "D9s48s3A2pVv"
      },
      "outputs": [],
      "source": [
        "#build the the prompts and invoke them.\n",
        "def build_and_act_4_0(recipe_filename, basepath=drive_path1, directory=\"Input\" ):\n",
        "  print(recipe_filename)\n",
        "  print(basepath)\n",
        "  print(directory)\n",
        "  role_for_model, context_for_model, prompts = build_recipe_path(recipe_filename,basepath)\n",
        "  act_on_prompts_4_0(role_for_model, context_for_model, prompts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbPngvVFa7m9"
      },
      "source": [
        "### Invoke the prompts and collect response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "xBOMuG6P2pNW"
      },
      "outputs": [],
      "source": [
        "#Invoke the model\n",
        "def act_on_prompts_4_0(role_for_model, context_for_model, prompts):\n",
        "    #print(prompts)\n",
        "    for item in prompts:\n",
        "      print(item)\n",
        "      #build individual prompts\n",
        "      #count_words(make_prompt(role_for_model, context_for_model, item))\n",
        "      response_list.append(get_response_from_model_4_0(make_prompt(role_for_model, context_for_model, item)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KH_qMaDMRsBJ"
      },
      "source": [
        "### Testing Model 4 with same recipes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4wJgchH2pad",
        "outputId": "f089cc2e-934c-49ac-9ed4-b0f92ba362aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recipe/InfraArchitect_recipe.json\n",
            "/gdrive/MyDrive/Gen-AI/Input\n",
            "Input\n",
            "recipe_filename:-Recipe/InfraArchitect_recipe.json\n",
            "Basepath:-/gdrive/MyDrive/Gen-AI/Input\n",
            "Joining path\n",
            "Path:- /gdrive/MyDrive/Gen-AI/Input/Recipe/InfraArchitect_recipe.json\n",
            "read_input_recepie processing \n",
            "Mounted at /gdrive\n",
            "Drive Path:/gdrive/MyDrive/Gen-AI/Input\n",
            "/gdrive/MyDrive/Gen-AI/Input/Recipe/InfraArchitect_recipe.json\n",
            "read_input_recepie processing finished:-\n",
            "{\"Role\": \"Cloud Infrastructure Architect\", \"Context\": \"In the context of a software integration project, two distinct systems play crucial roles: SAP and Commercetools SAAS. SAP, as the source system, is responsible for generating files, each of which may contain multiple records. Commercetools, on the other hand, is purpose-built to accept messages via its C# SDK.The project's primary objective is to leverage an Azure-based cloud platform, with a strong emphasis on Platform-as-a-Service (PaaS) solutions. This comprehensive approach encompasses essential components such as Azure API Management, Azure Storage for data storage, Azure Service Bus to ensure reliable messaging, Azure Function Apps for serverless computing, and Azure Logic Apps for seamless workflow orchestration.It's noteworthy that within the physical infrastructure, Azure, SAP, and Commercetools operate within distinct network segments, reinforcing the separation and integrity of each system's boundaries.\", \"Prompts\": [\"Suggest an Azure Infrastructure strategy to keep costs low.\", \"Suggest an Azure Infrastructure strategy for best security standards for each service.\", \"Suggest an Azure Infrastructure strategy to perform intrusion testing.\", \"Suggest list of risks and possible issues which may arise.\", \"Create an mermaid markup for the architectural components\", \"Generate Terraform Code to create Azure Service Bus with a topic, Azure API Management, Azure Storage, Azure Function App, Azure Logic App and hosted in a VNet with appropriate Subnet segregation.\"]}\n",
            "<class 'str'>\n",
            "parse_json processing\n",
            "<class 'str'>\n",
            "Mounted at /gdrive\n",
            "Drive Path:/gdrive/MyDrive/Gen-AI/Input\n",
            "/gdrive/MyDrive/Gen-AI/Input/Recipe/InfraArchitect_recipe.json\n",
            "{'Role': 'Cloud Infrastructure Architect', 'Context': \"In the context of a software integration project, two distinct systems play crucial roles: SAP and Commercetools SAAS. SAP, as the source system, is responsible for generating files, each of which may contain multiple records. Commercetools, on the other hand, is purpose-built to accept messages via its C# SDK.The project's primary objective is to leverage an Azure-based cloud platform, with a strong emphasis on Platform-as-a-Service (PaaS) solutions. This comprehensive approach encompasses essential components such as Azure API Management, Azure Storage for data storage, Azure Service Bus to ensure reliable messaging, Azure Function Apps for serverless computing, and Azure Logic Apps for seamless workflow orchestration.It's noteworthy that within the physical infrastructure, Azure, SAP, and Commercetools operate within distinct network segments, reinforcing the separation and integrity of each system's boundaries.\", 'Prompts': ['Suggest an Azure Infrastructure strategy to keep costs low.', 'Suggest an Azure Infrastructure strategy for best security standards for each service.', 'Suggest an Azure Infrastructure strategy to perform intrusion testing.', 'Suggest list of risks and possible issues which may arise.', 'Create an mermaid markup for the architectural components', 'Generate Terraform Code to create Azure Service Bus with a topic, Azure API Management, Azure Storage, Azure Function App, Azure Logic App and hosted in a VNet with appropriate Subnet segregation.']}\n",
            "Cloud Infrastructure Architect\n",
            "Suggest an Azure Infrastructure strategy to keep costs low.\n",
            "\n",
            "  ```Think like a/an Cloud Infrastructure Architect and and be as accurate as possible. In the context of a software integration project, two distinct systems play crucial roles: SAP and Commercetools SAAS. SAP, as the source system, is responsible for generating files, each of which may contain multiple records. Commercetools, on the other hand, is purpose-built to accept messages via its C# SDK.The project's primary objective is to leverage an Azure-based cloud platform, with a strong emphasis on Platform-as-a-Service (PaaS) solutions. This comprehensive approach encompasses essential components such as Azure API Management, Azure Storage for data storage, Azure Service Bus to ensure reliable messaging, Azure Function Apps for serverless computing, and Azure Logic Apps for seamless workflow orchestration.It's noteworthy that within the physical infrastructure, Azure, SAP, and Commercetools operate within distinct network segments, reinforcing the separation and integrity of each system's boundaries. and Suggest an Azure Infrastructure strategy to keep costs low. ```\n",
            "  \n",
            "To keep costs low while ensuring efficient integration between SAP and Commercetools SAAS, the following Azure Infrastructure strategy can be adopted:\n",
            "\n",
            "1. **Optimize Azure Storage**: Use Azure Blob Storage for storing large amounts of unstructured data. It's cost-effective and highly scalable. Implement lifecycle management policies to automatically move data to lower-cost tiers or delete it after a certain period.\n",
            "\n",
            "2. **Use Azure Functions**: Azure Functions, a serverless compute service, allows you to run your code without having to provision or manage infrastructure. This can significantly reduce costs as you only pay for the compute time you consume.\n",
            "\n",
            "3. **Implement Azure Logic Apps**: These can help in creating workflows without writing code, reducing development time and costs. They can also scale based on demand, ensuring you only pay for what you use.\n",
            "\n",
            "4. **Leverage Azure Service Bus**: This can help in decoupling applications and services from each other, leading to a more robust system. However, ensure to choose the right tier based on your needs to avoid unnecessary costs.\n",
            "\n",
            "5. **Use Azure API Management**: This can help in securing, publishing, and analyzing APIs, but it can be costly. Consider using the consumption tier for smaller workloads or the basic tier for larger ones to keep costs low.\n",
            "\n",
            "6. **Network Optimization**: Implement Azure Virtual Network to securely link Azure resources to each other. Use Azure ExpressRoute for private connections between Azure datacenters and infrastructure on your premises or in a colocation environment to reduce bandwidth costs.\n",
            "\n",
            "7. **Monitor and Optimize**: Use Azure Cost Management and Azure Advisor to monitor your usage and get personalized recommendations for optimizing your resources and reducing costs.\n",
            "\n",
            "8. **Scale on Demand**: Use Azure's auto-scaling feature to automatically scale up or down based on demand, ensuring you only pay for what you need.\n",
            "\n",
            "9. **Reserve Instances**: If you have predictable workloads, consider reserving instances. Azure Reserved VM Instances can save you up to 72% over pay-as-you-go prices.\n",
            "\n",
            "10. **Clean Up Unused Resources**: Regularly review and remove unused or underutilized resources to avoid unnecessary costs. \n",
            "\n",
            "Remember, the key to cost optimization in Azure is continuous monitoring and optimization.\n",
            "Suggest an Azure Infrastructure strategy for best security standards for each service.\n",
            "\n",
            "  ```Think like a/an Cloud Infrastructure Architect and and be as accurate as possible. In the context of a software integration project, two distinct systems play crucial roles: SAP and Commercetools SAAS. SAP, as the source system, is responsible for generating files, each of which may contain multiple records. Commercetools, on the other hand, is purpose-built to accept messages via its C# SDK.The project's primary objective is to leverage an Azure-based cloud platform, with a strong emphasis on Platform-as-a-Service (PaaS) solutions. This comprehensive approach encompasses essential components such as Azure API Management, Azure Storage for data storage, Azure Service Bus to ensure reliable messaging, Azure Function Apps for serverless computing, and Azure Logic Apps for seamless workflow orchestration.It's noteworthy that within the physical infrastructure, Azure, SAP, and Commercetools operate within distinct network segments, reinforcing the separation and integrity of each system's boundaries. and Suggest an Azure Infrastructure strategy for best security standards for each service. ```\n",
            "  \n",
            "To ensure the best security standards for each service in the Azure Infrastructure, the following strategies can be implemented:\n",
            "\n",
            "1. **Azure API Management**: Implement Azure API Management to secure APIs with built-in authentication and authorization support. Use Azure Active Directory for identity management and OAuth 2.0 protocol for securing APIs. Enable IP filtering to restrict access to the API management service from specific IP addresses.\n",
            "\n",
            "2. **Azure Storage**: Use Azure Storage Service Encryption (SSE) for data at rest. It automatically encrypts data prior to storing it and decrypts it prior to retrieval. Implement Azure Role-Based Access Control (RBAC) to manage who has access to Azure resources and what they can do with them. Use Shared Access Signatures (SAS) to provide secure, delegated access to resources in your storage account.\n",
            "\n",
            "3. **Azure Service Bus**: Enable Azure Service Bus Geo-disaster recovery to replicate and failover messaging entities across different regions. Use Shared Access Signature (SAS) authentication, which provides access to all messaging operations.\n",
            "\n",
            "4. **Azure Function Apps**: Use Azure Functions secure environment variables (app settings) to store secrets. Enable Azure Functions network isolation with Virtual Network (VNet) to restrict inbound traffic to your function app. \n",
            "\n",
            "5. **Azure Logic Apps**: Use Azure Logic Apps to create secure workflows with built-in security and auditing. Implement Azure Managed Identity to securely access other resources that support Azure AD authentication.\n",
            "\n",
            "6. **Network Security**: Implement Azure Network Security Groups (NSGs) to control inbound and outbound traffic to Azure resources. Use Azure Firewall, a managed, cloud-based network security service to protect your Azure Virtual Network resources.\n",
            "\n",
            "7. **Identity and Access Management**: Implement Azure Active Directory for identity management. Use Multi-Factor Authentication for an extra layer of security.\n",
            "\n",
            "8. **Monitoring and Auditing**: Use Azure Security Center for unified security management and advanced threat protection. Implement Azure Monitor to collect, analyze, and act on telemetry data from your Azure and on-premises environments.\n",
            "\n",
            "9. **Data Protection**: Implement Azure Backup for data protection and disaster recovery in the cloud. Use Azure Site Recovery for disaster recovery.\n",
            "\n",
            "10. **Compliance**: Use Azure Policy for implementing governance for resource consistency, regulatory compliance, security, cost, and management.\n",
            "\n",
            "Remember, security is not a one-time activity but an ongoing process. Regularly review and update your security configurations and practices, and always stay informed about the latest security features and best practices in Azure.\n",
            "Suggest an Azure Infrastructure strategy to perform intrusion testing.\n",
            "\n",
            "  ```Think like a/an Cloud Infrastructure Architect and and be as accurate as possible. In the context of a software integration project, two distinct systems play crucial roles: SAP and Commercetools SAAS. SAP, as the source system, is responsible for generating files, each of which may contain multiple records. Commercetools, on the other hand, is purpose-built to accept messages via its C# SDK.The project's primary objective is to leverage an Azure-based cloud platform, with a strong emphasis on Platform-as-a-Service (PaaS) solutions. This comprehensive approach encompasses essential components such as Azure API Management, Azure Storage for data storage, Azure Service Bus to ensure reliable messaging, Azure Function Apps for serverless computing, and Azure Logic Apps for seamless workflow orchestration.It's noteworthy that within the physical infrastructure, Azure, SAP, and Commercetools operate within distinct network segments, reinforcing the separation and integrity of each system's boundaries. and Suggest an Azure Infrastructure strategy to perform intrusion testing. ```\n",
            "  \n",
            "As a Cloud Infrastructure Architect, I would suggest the following Azure Infrastructure strategy for performing intrusion testing:\n",
            "\n",
            "1. **Azure Security Center**: Utilize Azure Security Center for continuous security assessment and actionable security recommendations. It provides advanced threat protection across your hybrid workloads in the cloud.\n",
            "\n",
            "2. **Azure Penetration Testing**: Azure allows and encourages penetration testing on its platform. You can conduct penetration testing on your applications hosted in Azure without needing to contact Azure. However, for large-scale penetration testing, you need to obtain approval from Azure.\n",
            "\n",
            "3. **Azure Network Watcher**: Use Azure Network Watcher for monitoring and diagnosing conditions at a network scenario level in, to, and from Azure. Network diagnostic and visualization tools available with Network Watcher help you understand, diagnose, and gain insights to your network in Azure.\n",
            "\n",
            "4. **Azure Application Gateway Web Application Firewall (WAF)**: Use Azure Application Gateway WAF to protect web applications from common web-based attacks like SQL injection, cross-site scripting attacks, and session hijacks. It comes pre-configured with OWASP modsecurity core rule set protecting against common attacks.\n",
            "\n",
            "5. **Azure DDoS Protection**: Use Azure DDoS Protection to protect your Azure applications from Distributed Denial of Service (DDoS) attacks. It provides always-on traffic monitoring and real-time mitigation of common network-level attacks.\n",
            "\n",
            "6. **Azure Advanced Threat Protection (ATP)**: Use Azure ATP to detect and investigate advanced attacks on-premises and in the cloud. The service uses machine learning, analytics and unique Microsoft optics across the Windows ecosystem to make informed decisions.\n",
            "\n",
            "7. **Azure Log Analytics**: Use Azure Log Analytics for collecting and analyzing data generated by resources in your cloud and on-premises environments. It gives you real-time insights using integrated search and custom dashboards to readily analyze millions of records across all your workloads and servers.\n",
            "\n",
            "Remember, before performing any intrusion testing, ensure you have a well-defined incident response plan in place. This plan should include steps to take when an intrusion is detected, including how to contain and eliminate the threat, and how to recover any compromised data.\n",
            "Suggest list of risks and possible issues which may arise.\n",
            "\n",
            "  ```Think like a/an Cloud Infrastructure Architect and and be as accurate as possible. In the context of a software integration project, two distinct systems play crucial roles: SAP and Commercetools SAAS. SAP, as the source system, is responsible for generating files, each of which may contain multiple records. Commercetools, on the other hand, is purpose-built to accept messages via its C# SDK.The project's primary objective is to leverage an Azure-based cloud platform, with a strong emphasis on Platform-as-a-Service (PaaS) solutions. This comprehensive approach encompasses essential components such as Azure API Management, Azure Storage for data storage, Azure Service Bus to ensure reliable messaging, Azure Function Apps for serverless computing, and Azure Logic Apps for seamless workflow orchestration.It's noteworthy that within the physical infrastructure, Azure, SAP, and Commercetools operate within distinct network segments, reinforcing the separation and integrity of each system's boundaries. and Suggest list of risks and possible issues which may arise. ```\n",
            "  \n",
            "1. **Integration Complexity**: The integration of SAP and Commercetools SAAS via Azure could be complex due to the different architectures of the systems. This could lead to potential issues in data consistency and synchronization.\n",
            "\n",
            "2. **Data Security**: As data is transferred between different systems and stored in Azure Storage, there could be potential risks related to data security and privacy. \n",
            "\n",
            "3. **Performance**: The performance of the integrated system could be affected due to the latency in data transfer between different systems and Azure services.\n",
            "\n",
            "4. **Cost**: The cost of using Azure services could be high, especially if the data volume is large and the usage of services like Azure API Management, Azure Storage, Azure Service Bus, Azure Function Apps, and Azure Logic Apps is extensive.\n",
            "\n",
            "5. **Reliability**: The reliability of Azure services could be a concern. Any downtime or performance issues with Azure services could impact the overall system performance.\n",
            "\n",
            "6. **Scalability**: The scalability of the integrated system could be a concern. If the data volume increases significantly, the system should be able to handle it without any performance degradation.\n",
            "\n",
            "7. **Data Loss**: There is a risk of data loss during the data transfer between different systems and Azure services.\n",
            "\n",
            "8. **Compliance**: Compliance with data protection regulations could be a challenge as data is stored and processed in Azure.\n",
            "\n",
            "9. **Vendor Lock-in**: There is a risk of vendor lock-in with Azure. If the organization decides to switch to a different cloud provider in the future, it could be a complex and costly process.\n",
            "\n",
            "10. **Resource Management**: Managing resources across different systems and Azure services could be challenging and could lead to potential issues in resource allocation and utilization.\n",
            "\n",
            "11. **Network Segmentation**: As Azure, SAP, and Commercetools operate within distinct network segments, there could be potential issues related to network connectivity and data transfer. \n",
            "\n",
            "12. **Skill Gap**: There could be a skill gap in the team in terms of understanding and managing Azure services and the integration of SAP and Commercetools SAAS. This could lead to potential issues in system implementation and management.\n",
            "Create an mermaid markup for the architectural components\n",
            "\n",
            "  ```Think like a/an Cloud Infrastructure Architect and and be as accurate as possible. In the context of a software integration project, two distinct systems play crucial roles: SAP and Commercetools SAAS. SAP, as the source system, is responsible for generating files, each of which may contain multiple records. Commercetools, on the other hand, is purpose-built to accept messages via its C# SDK.The project's primary objective is to leverage an Azure-based cloud platform, with a strong emphasis on Platform-as-a-Service (PaaS) solutions. This comprehensive approach encompasses essential components such as Azure API Management, Azure Storage for data storage, Azure Service Bus to ensure reliable messaging, Azure Function Apps for serverless computing, and Azure Logic Apps for seamless workflow orchestration.It's noteworthy that within the physical infrastructure, Azure, SAP, and Commercetools operate within distinct network segments, reinforcing the separation and integrity of each system's boundaries. and Create an mermaid markup for the architectural components ```\n",
            "  \n",
            "Here is a simple mermaid markup for the architectural components:\n",
            "\n",
            "```mermaid\n",
            "graph LR\n",
            "    A[SAP] -->|Generates Files| B[Azure Storage]\n",
            "    B -->|Sends Messages| C[Azure Service Bus]\n",
            "    C -->|Triggers| D[Azure Function Apps]\n",
            "    D -->|Calls| E[Azure API Management]\n",
            "    E -->|Sends Messages| F[Commercetools]\n",
            "    D -->|Orchestrates Workflow| G[Azure Logic Apps]\n",
            "```\n",
            "\n",
            "This diagram represents the flow of data and interactions between different components. SAP generates files that are stored in Azure Storage. These files trigger messages on the Azure Service Bus, which in turn trigger Azure Function Apps. The Function Apps call Azure API Management, which sends messages to Commercetools. The Azure Function Apps also orchestrate workflows using Azure Logic Apps. \n",
            "\n",
            "Please note that this is a simplified representation and the actual architecture may be more complex depending on the specific requirements of the project.\n",
            "Generate Terraform Code to create Azure Service Bus with a topic, Azure API Management, Azure Storage, Azure Function App, Azure Logic App and hosted in a VNet with appropriate Subnet segregation.\n",
            "\n",
            "  ```Think like a/an Cloud Infrastructure Architect and and be as accurate as possible. In the context of a software integration project, two distinct systems play crucial roles: SAP and Commercetools SAAS. SAP, as the source system, is responsible for generating files, each of which may contain multiple records. Commercetools, on the other hand, is purpose-built to accept messages via its C# SDK.The project's primary objective is to leverage an Azure-based cloud platform, with a strong emphasis on Platform-as-a-Service (PaaS) solutions. This comprehensive approach encompasses essential components such as Azure API Management, Azure Storage for data storage, Azure Service Bus to ensure reliable messaging, Azure Function Apps for serverless computing, and Azure Logic Apps for seamless workflow orchestration.It's noteworthy that within the physical infrastructure, Azure, SAP, and Commercetools operate within distinct network segments, reinforcing the separation and integrity of each system's boundaries. and Generate Terraform Code to create Azure Service Bus with a topic, Azure API Management, Azure Storage, Azure Function App, Azure Logic App and hosted in a VNet with appropriate Subnet segregation. ```\n",
            "  \n",
            "As a Cloud Infrastructure Architect, I would approach this project by first understanding the data flow and integration points between SAP and Commercetools. The Azure-based cloud platform will serve as the intermediary, facilitating the transfer of data from SAP to Commercetools.\n",
            "\n",
            "Here's a high-level overview of how the architecture might look:\n",
            "\n",
            "1. SAP generates files containing multiple records. These files are stored in Azure Storage, which provides scalable and secure data storage.\n",
            "\n",
            "2. Azure Function Apps, which provide serverless computing, are triggered by the arrival of new files in Azure Storage. These Function Apps read the files, process the records, and transform them into messages that can be understood by Commercetools.\n",
            "\n",
            "3. The messages are sent to Azure Service Bus, a reliable cloud messaging service. A topic in the Service Bus is used to decouple the sending and receiving of messages.\n",
            "\n",
            "4. Azure Logic Apps, which provide seamless workflow orchestration, are triggered by the arrival of new messages in the Service Bus. These Logic Apps take the messages and send them to Commercetools via its C# SDK.\n",
            "\n",
            "5. Azure API Management is used to manage and secure the APIs that are used in this process.\n",
            "\n",
            "6. All these components are hosted in an Azure Virtual Network (VNet) with appropriate subnet segregation to ensure the separation and integrity of each system's boundaries.\n",
            "\n",
            "As for generating the Terraform code, it would be quite extensive and beyond the scope of this response. However, Terraform's documentation and the Azure Provider documentation provide excellent resources for writing the code. You would need to write Terraform code for each of the Azure services mentioned above (API Management, Storage, Service Bus, Function Apps, Logic Apps, and VNet). Each of these services has its own set of Terraform resources that you would need to define and configure according to your project's requirements.\n"
          ]
        }
      ],
      "source": [
        "#build_and_act_4_0(\"Recipe/Business_Analyst_recipe.json\")\n",
        "#build_and_act_4_0(\"GCSE_Year11.json\")\n",
        "#build_and_act_4_0(\"GCSE_Year11_Recipe.json\")\n",
        "#build_recipe_path(drive_path1, \"Input\", \"Infra_Architect_recipe.json\")\n",
        "build_and_act_4_0(\"Recipe/InfraArchitect_recipe.json\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emit_output(response_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Irj8x9HIG93W",
        "outputId": "74ee778a-882e-4fcb-825e-1abae6b11691"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As a Cloud Infrastructure Architect, I would design the integration between SAP and Commercetools using the Azure-based cloud platform with PAAS offerings. Here is a high-level overview of the architecture:\n",
            "\n",
            "1. SAP Integration:\n",
            "   - SAP will emit files containing multiple records.\n",
            "   - These files can be stored in Azure Storage for further processing.\n",
            "   - Azure Function Apps can be used to monitor the storage container and trigger processing whenever new files are added.\n",
            "   - The Function App can read the SAP files, extract the necessary data, and transform it into a format suitable for integration with Commercetools.\n",
            "   - The transformed data can be sent to the Service Bus messaging system for further processing.\n",
            "\n",
            "2. Commercetools Integration:\n",
            "   - The C# SDK for Commercetools can be used to accept messages from the Service Bus.\n",
            "   - Azure Function Apps can be used to process the messages received from the Service Bus and perform any necessary actions in Commercetools.\n",
            "   - The Function App can utilize the C# SDK to interact with the Commercetools API and perform operations such as creating products, updating inventory, etc.\n",
            "\n",
            "3. API Management:\n",
            "   - Azure API Management can be used to expose APIs for both SAP and Commercetools integration.\n",
            "   - It can provide a unified interface for external systems to interact with the integrated solution.\n",
            "   - API Management can handle authentication, authorization, rate limiting, and other API management functionalities.\n",
            "\n",
            "4. Networking and Security:\n",
            "   - The entire solution can be hosted within a Virtual Network (VNet) for enhanced security and isolation.\n",
            "   - The VNet can be divided into appropriate subnets to segregate different components of the solution.\n",
            "   - Network Security Groups (NSGs) can be used to control inbound and outbound traffic to the subnets.\n",
            "   - Access to the resources within the VNet can be further restricted using Azure Private Link.\n",
            "\n",
            "To generate a Terraform code for creating the required Azure resources, you can use the following steps:\n",
            "\n",
            "1. Define the Azure Resource Group and location.\n",
            "2. Create an Azure Storage Account for storing SAP files.\n",
            "3. Create an Azure Function App for processing SAP files and sending messages to the Service Bus.\n",
            "4. Create an Azure Service Bus Namespace and Queue/Topic for message communication.\n",
            "5. Create an Azure Function App for processing messages from the Service Bus and interacting with Commercetools.\n",
            "6. Create an Azure API Management instance and configure APIs for SAP and Commercetools integration.\n",
            "7. Configure the VNet and subnets for hosting the solution components.\n",
            "8. Associate the Function Apps and API Management with the appropriate subnets.\n",
            "9. Configure NSGs and Azure Private Link for network security and isolation.\n",
            "\n",
            "By following these steps, you can create the necessary Azure resources using Terraform to implement the integration between SAP and Commercetools on the Azure cloud platform.\n",
            "As a Cloud Infrastructure Architect, I would suggest the following approach:\n",
            "\n",
            "1. **Azure Service Bus**: This will be used as a messaging system to decouple SAP and Commercetools. SAP will emit files that will be sent as messages to the Azure Service Bus. \n",
            "\n",
            "2. **Azure Function App**: This will be triggered whenever a new message arrives in the Azure Service Bus. The Function App will process the message and send it to Commercetools using the C# SDK.\n",
            "\n",
            "3. **Azure API Management**: This will be used to manage and secure the APIs used by the Azure Function App to communicate with Commercetools.\n",
            "\n",
            "4. **Azure Storage**: This will be used to store the files emitted by SAP before they are sent as messages to the Azure Service Bus.\n",
            "\n",
            "5. **VNet and Subnets**: All these services will be hosted in a VNet with appropriate subnets to ensure network isolation and security.\n",
            "\n",
            "Here is a sample Terraform code to create these resources:\n",
            "\n",
            "```hcl\n",
            "provider \"azurerm\" {\n",
            "  features {}\n",
            "}\n",
            "\n",
            "resource \"azurerm_resource_group\" \"example\" {\n",
            "  name     = \"example-resources\"\n",
            "  location = \"West Europe\"\n",
            "}\n",
            "\n",
            "resource \"azurerm_servicebus_namespace\" \"example\" {\n",
            "  name                = \"example-servicebus\"\n",
            "  location            = azurerm_resource_group.example.location\n",
            "  resource_group_name = azurerm_resource_group.example.name\n",
            "  sku                 = \"Standard\"\n",
            "}\n",
            "\n",
            "resource \"azurerm_api_management\" \"example\" {\n",
            "  name                = \"example-apim\"\n",
            "  location            = azurerm_resource_group.example.location\n",
            "  resource_group_name = azurerm_resource_group.example.name\n",
            "  publisher_name      = \"My Company\"\n",
            "  publisher_email     = \"company@example.com\"\n",
            "}\n",
            "\n",
            "resource \"azurerm_storage_account\" \"example\" {\n",
            "  name                     = \"examplestorageacc\"\n",
            "  resource_group_name      = azurerm_resource_group.example.name\n",
            "  location                 = azurerm_resource_group.example.location\n",
            "  account_tier             = \"Standard\"\n",
            "  account_replication_type = \"GRS\"\n",
            "}\n",
            "\n",
            "resource \"azurerm_function_app\" \"example\" {\n",
            "  name                       = \"example-functionapp\"\n",
            "  location                   = azurerm_resource_group.example.location\n",
            "  resource_group_name        = azurerm_resource_group.example.name\n",
            "  app_service_plan_id        = azurerm_app_service_plan.example.id\n",
            "  storage_connection_string  = azurerm_storage_account.example.primary_connection_string\n",
            "}\n",
            "\n",
            "resource \"azurerm_virtual_network\" \"example\" {\n",
            "  name                = \"example-network\"\n",
            "  resource_group_name = azurerm_resource_group.example.name\n",
            "  location            = azurerm_resource_group.example.location\n",
            "  address_space       = [\"10.0.0.0/16\"]\n",
            "}\n",
            "\n",
            "resource \"azurerm_subnet\" \"example\" {\n",
            "  name                 = \"internal\"\n",
            "  resource_group_name  = azurerm_resource_group.example.name\n",
            "  virtual_network_name = azurerm_virtual_network.example.name\n",
            "  address_prefix       = \"10.0.2.0/24\"\n",
            "}\n",
            "```\n",
            "\n",
            "Please replace the placeholders with your actual values. This is a basic setup and might need to be adjusted based on your specific requirements and constraints.\n",
            "To keep costs low while ensuring efficient integration between SAP and Commercetools SAAS, the following Azure Infrastructure strategy can be adopted:\n",
            "\n",
            "1. **Azure API Management**: Use Azure API Management to create consistent and modern API gateways for existing back-end services hosted anywhere, secure and protect them from abuse and overuse, and get insights into usage and health. This will help in managing and securing the APIs effectively.\n",
            "\n",
            "2. **Azure Service Bus**: Use Azure Service Bus for asynchronous data transfer between SAP and Commercetools. Service Bus is a fully managed enterprise integration message broker. Service Bus can decouple applications and services. Service Bus offers a reliable and secure platform for asynchronous transfer of data and state.\n",
            "\n",
            "3. **Azure Function Apps**: Use Azure Function Apps to process the files produced by SAP. Azure Functions is a serverless solution that allows you to write less code, maintain less infrastructure, and save on costs. Instead of worrying about deploying and maintaining servers, the cloud infrastructure provides all the up-to-date resources needed to keep your applications running.\n",
            "\n",
            "4. **Azure Logic Apps**: Use Azure Logic Apps for orchestrating and coordinating the data flow between SAP and Commercetools. Logic Apps simplifies how you design and build scalable solutions for app integration, data integration, system integration, enterprise application integration (EAI), and business-to-business (B2B) communication.\n",
            "\n",
            "5. **Cost Management**: Use Azure Cost Management and Billing to monitor, control, and optimize your Azure costs. It provides tools to monitor, allocate, and optimize costs, empowering you to do more with less.\n",
            "\n",
            "6. **Optimize Resource Usage**: Make sure to optimize the usage of Azure resources. For instance, you can schedule your Azure Function Apps to run only during the times when SAP produces files. This way, you won't be charged when the functions are not running.\n",
            "\n",
            "7. **Choose the Right Tier**: Choose the right pricing tier for each Azure service you use. For example, if your workload is not very heavy, you can choose a lower tier for Azure Service Bus and Azure API Management.\n",
            "\n",
            "8. **Data Transfer**: Minimize data transfer costs by reducing the amount of data sent between SAP and Commercetools. You can do this by optimizing the file format produced by SAP and only sending the necessary data.\n",
            "\n",
            "9. **Auto Scaling**: Use Azure's auto-scaling feature to automatically scale up or down the number of compute resources that are being allocated to your application based on its needs at any given time. This way, you're only paying for what you use.\n",
            "\n",
            "10. **Use Reserved Instances**: If you have predictable workloads, you can use Azure Reserved Instances to reserve resources in advance and save on costs.\n",
            "1. **API Management**: Azure API Management should be configured to use Azure Active Directory (Azure AD) for authentication. This will ensure that only authenticated and authorized users can access the APIs. Additionally, you should enable IP filtering to restrict access to the API Management service from specific IP addresses. You should also enable HTTPS to ensure secure communication.\n",
            "\n",
            "2. **Service Bus Messaging**: For Azure Service Bus, you should use Shared Access Signature (SAS) authentication, which provides a secure way to authenticate and authorize users. You should also enable encryption at rest and in transit to protect the data. \n",
            "\n",
            "3. **Azure Function Apps**: Azure Function Apps should be configured to use Azure AD for authentication. You should also enable Managed Service Identity (MSI) to securely access other Azure services. Additionally, you should enable HTTPS only mode to ensure secure communication.\n",
            "\n",
            "4. **Azure Logic Apps**: For Azure Logic Apps, you should use Azure AD for authentication. You should also enable IP filtering to restrict access to the Logic Apps from specific IP addresses. Additionally, you should enable HTTPS to ensure secure communication.\n",
            "\n",
            "5. **Data Transfer**: For transferring files from SAP to Azure, you should use Azure Data Factory with Secure Sockets Layer (SSL) or Transport Layer Security (TLS) for secure data transfer. \n",
            "\n",
            "6. **Commercetools SAAS**: For Commercetools, you should use its C# SDK with Azure AD for authentication. You should also enable HTTPS to ensure secure communication.\n",
            "\n",
            "7. **Network Security**: You should use Azure Virtual Network (VNet) to isolate your Azure resources from the public internet. You should also use Network Security Groups (NSGs) to control inbound and outbound traffic to your Azure resources.\n",
            "\n",
            "8. **Identity and Access Management**: You should use Azure AD for identity and access management. You should also implement role-based access control (RBAC) to ensure that users have the minimum necessary permissions.\n",
            "\n",
            "9. **Monitoring and Auditing**: You should use Azure Monitor and Azure Security Center to monitor your Azure resources and detect any security threats. You should also enable Azure Activity Log to audit activities in your Azure environment. \n",
            "\n",
            "10. **Data Protection**: You should use Azure Backup and Azure Site Recovery to protect your data and ensure business continuity in case of any disaster. \n",
            "\n",
            "11. **Compliance**: You should use Azure Policy and Azure Blueprints to enforce compliance with your organization's security policies and standards.\n",
            "The Azure Infrastructure strategy for intrusion testing in this scenario would involve several steps:\n",
            "\n",
            "1. **Environment Setup**: Create a separate testing environment that mirrors the production environment. This includes the SAP system, Commercetools SAAS, Azure API Management, Service Bus messaging, Azure Function Apps, and Azure Logic Apps. \n",
            "\n",
            "2. **Define Test Scenarios**: Identify potential intrusion scenarios. This could include unauthorized access to the SAP system, Commercetools SAAS, or the Azure services. Also, consider scenarios where the file transfer from SAP to Commercetools is intercepted or manipulated.\n",
            "\n",
            "3. **Implement Intrusion Tests**: Use Azure Security Center to implement intrusion tests. This could involve simulating attacks on the SAP system, Commercetools SAAS, or the Azure services. Monitor the system's response to these attacks.\n",
            "\n",
            "4. **Monitor and Analyze**: Use Azure Monitor and Azure Log Analytics to monitor the system during the intrusion tests. Analyze the logs to identify any vulnerabilities or weaknesses.\n",
            "\n",
            "5. **Implement Security Measures**: Based on the results of the intrusion tests, implement necessary security measures. This could include configuring Azure Security Center policies, implementing Azure Active Directory for identity and access management, and encrypting the file transfer from SAP to Commercetools.\n",
            "\n",
            "6. **Continuous Testing and Monitoring**: Regularly perform intrusion tests to ensure the security measures are effective. Use Azure Monitor and Azure Log Analytics for continuous monitoring and alerting of potential security threats.\n",
            "\n",
            "7. **Documentation and Reporting**: Document all the test cases, findings, and remediation steps. Generate reports using Azure Security Center for auditing and compliance purposes.\n",
            "\n",
            "Remember, intrusion testing should be performed by qualified security professionals and should be part of a comprehensive security strategy. Always follow ethical guidelines and legal requirements when performing intrusion testing.\n",
            "As a Cloud Infrastructure Architect, I would approach this project in the following way:\n",
            "\n",
            "1. **Azure Service Bus with a Topic**: The Azure Service Bus is a fully managed enterprise integration message broker. It can decouple applications and services. A topic can be used to send messages from one application and distribute them to multiple independent applications. In this case, SAP can produce files and send them as messages to the Service Bus topic.\n",
            "\n",
            "2. **Azure API Management**: This is a turnkey solution for publishing APIs to external and internal customers. It can be used to wrap the Commercetools SAAS API, providing a layer of abstraction and control. This can help in managing and securing the API, as well as monitoring its usage.\n",
            "\n",
            "3. **Azure Storage**: This is a Microsoft-managed service providing cloud storage that is highly available, secure, durable, scalable, and redundant. It can be used to store the files produced by SAP before they are sent as messages to the Service Bus.\n",
            "\n",
            "4. **Azure Function App**: This is a serverless compute service that lets you run event-triggered code without having to explicitly provision or manage infrastructure. It can be used to process the messages from the Service Bus, perhaps transforming the data or performing some other kind of processing before sending it to Commercetools.\n",
            "\n",
            "5. **Azure Logic App**: This is a cloud service that helps you schedule, automate, and orchestrate tasks, business processes, and workflows when you need to integrate apps, data, systems, and services across enterprises or organizations. It can be used to orchestrate the overall flow of data from SAP to Commercetools.\n",
            "\n",
            "6. **VNet with appropriate Subnet segregation**: Azure Virtual Network (VNet) is the fundamental building block for your private network in Azure. It enables many types of Azure resources, such as Azure Virtual Machines (VM), to securely communicate with each other, the internet, and on-premises networks. Subnet segregation can be used to isolate the different components of the solution for security and management purposes.\n",
            "\n",
            "7. **Terraform Code**: Terraform is an open-source infrastructure as code software tool that provides a consistent CLI workflow to manage hundreds of cloud services. Terraform codifies APIs into declarative configuration files. This code can be shared amongst team members, treated as code, edited, reviewed, and versioned. In this case, Terraform code will be used to create and manage the Azure resources mentioned above.\n",
            "\n",
            "This approach will ensure a robust, scalable, and secure integration between SAP and Commercetools.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbzvgDFVQd28"
      },
      "outputs": [],
      "source": [
        "loader = PyPDFLoader(\"PITLDNCC.pdf\")\n",
        "pages = loader.load_and_split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zltu5_QDa4vj"
      },
      "outputs": [],
      "source": [
        "data = pages\n",
        "pages = pages.count\n",
        "print(pages)\n",
        "print(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFxZpcPnnt88"
      },
      "outputs": [],
      "source": [
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3TKRbbSn_V7"
      },
      "outputs": [],
      "source": [
        "print(documents.count)\n",
        "\n",
        "#for index, item in enumerate(documents):\n",
        "    #print(f\"Index {index}: {item}\")\n",
        "   # print(\"index - \" + index)\n",
        "    #print(count_words(documents[index].page_content))\n",
        "#act_on_prompts_4_0(\"C# Application Developer\",documents[0].page_content,\"Generate C# code for every statement which matches requirement in this context. Keep seperate for method and follow best practices.\")\n",
        "get_response_from_model_4_0(make_prompt(\"C# Application Developer\", documents[0].page_content, \"Generate C# code for every requirement in this context. Use latest dotnet template and utilise BackgroundService. Keep seperate method for every requirement, add code comments and follow best practices.\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pM2EyftDnvN5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8UTQlMtnvUq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4iFFyxudxdJ"
      },
      "outputs": [],
      "source": [
        "type(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdEzGui7d7yE"
      },
      "outputs": [],
      "source": [
        "for item in data:\n",
        "    print(item)\n",
        "    regex = \"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\"\n",
        "    print(type(item))\n",
        "\n",
        "\n",
        "    #response = re.split(regex, item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asQSADFlo6mg"
      },
      "outputs": [],
      "source": [
        "for index, item in enumerate(data):\n",
        "    print(f\"Index {index}: {item}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRBuvf0hdJmt"
      },
      "outputs": [],
      "source": [
        "\n",
        "import regex as re\n",
        "regex = \"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\"\n",
        "re.split(regex, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMHNiOYnOgbH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6OYGs1elBOQ"
      },
      "outputs": [],
      "source": [
        "data = {\n",
        "    \"name\": \"John\",\n",
        "    \"age\": 30,\n",
        "    \"city\": \"New York\",\n",
        "    \"pets\": [\n",
        "        {\"type\": \"dog\", \"name\": \"Fido\"},\n",
        "        {\"type\": \"cat\", \"name\": \"Fluffy\"}\n",
        "    ]\n",
        "}\n",
        "\n",
        "pprint.pprint(data, width=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JwI_9DbmX0E"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "# Markdown text\n",
        "markdown_text = \"\"\"\n",
        "# Heading 1\n",
        "## Heading 2\n",
        "\n",
        "**Bold Text** or __Bold Text__\n",
        "*Italic Text* or _Italic Text_\n",
        "\n",
        "- Bullet Point 1\n",
        "- Bullet Point 2\n",
        "\n",
        "[Link to Google](https://www.google.com/)\n",
        "\n",
        "![Image Alt Text](image_url.jpg)\n",
        "\"\"\"\n",
        "\n",
        "# Render the Markdown text\n",
        "Markdown(markdown_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "gu7Lvpv8v7iE",
        "outputId": "7f4136dd-2c04-4e4d-9f76-4b8d3ebe80da"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h1>Heading 1</h1>\n",
              "<p>This is a <strong>bold</strong> and <em>italic</em> text.</p>\n",
              "<ul>\n",
              "<li>Item 1</li>\n",
              "<li>Item 2</li>\n",
              "<li>Item 3</li>\n",
              "</ul>\n",
              "<p>A <a href=\"https://www.example.com\">link</a>.</p>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "import markdown\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Your Markdown text\n",
        "markdown_text = \"\"\"\n",
        "# Heading 1\n",
        "\n",
        "This is a **bold** and *italic* text.\n",
        "\n",
        "- Item 1\n",
        "- Item 2\n",
        "- Item 3\n",
        "\n",
        "A [link](https://www.example.com).\n",
        "\"\"\"\n",
        "\n",
        "# Convert Markdown to HTML\n",
        "html_text = markdown.markdown(markdown_text)\n",
        "\n",
        "# Display the HTML\n",
        "HTML(html_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "id": "FwVcLtZmsdUR",
        "outputId": "363fa4d9-53d2-4ddc-bb8c-8f00334a8d33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (3.4.4)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h1>Heading 1</h1>\n",
              "<p>This is a <strong>bold</strong> and <em>italic</em> text.1\n",
              "- Item 1\n",
              "- Item 2\n",
              "- Item 3\n",
              "A <a href=\"https://www.example.com\">link</a>.</p>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "!pip install markdown\n",
        "import markdown\n",
        "\n",
        "# Your input text\n",
        "input_text = \"\"\"\n",
        "# Heading 1\n",
        "This is a **bold** and *italic* text.1\n",
        "- Item 1\n",
        "- Item 2\n",
        "- Item 3\n",
        "A [link](https://www.example.com).\n",
        "\"\"\"\n",
        "\n",
        "# Convert to Markdown\n",
        "markdown_text = markdown.markdown(input_text)\n",
        "#html_text = markdown.markdown(markdown_text)\n",
        "\n",
        "# Display the HTML\n",
        "HTML(markdown_text)\n",
        "# Print the Markdown\n",
        "#print(markdown_text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ryBuWNAdjp-0_U_50W53SKZZwRTbVXp-",
      "authorship_tag": "ABX9TyNN8ya1BQpztFVMp/erD4xs",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}